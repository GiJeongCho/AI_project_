{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Happy\\Desktop\\AI_project\\자격증_빅분기 토스 _AICE\\빅분기\\input\"\n",
    "# .iloc로 하기 df[0] => 인덱스로 확인함.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn 모듈의 tree import\n",
    "from sklearn import tree\n",
    "\n",
    "# 간단한 데이터셋 생성\n",
    "X = [[0, 0], [1, 1]]               \n",
    "Y = [0, 1]\n",
    "\n",
    "# 의사결정나무 적합 및 학습데이터 예측\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict([[1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train[\"Survived\"]) # 벨류셋 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(x_train,y_train[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "Y.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-1\n",
    "생존여부 예측모델 만들기\n",
    "학습용 데이터 (X_train, y_train)을 이용하여 생존 예측 모형을 만든 후, 이를 평가용 데이터(X_test)에 적용하여 얻은 예측값을 다음과 같은 형식의 CSV파일로 생성하시오(제출한 모델의 성능은 accuracy 평가지표에 따라 채점)\n",
    "(가) 제공 데이터 목록\n",
    "\n",
    "y_train: 생존여부(학습용)\n",
    "X_trian, X_test : 승객 정보 (학습용 및 평가용)\n",
    "(나) 데이터 형식 및 내용\n",
    "\n",
    "y_trian (712명 데이터)\n",
    "시험환경 세팅은 예시문제와 동일한 형태의 X_train, y_train, X_test 데이터를 만들기 위함임\n",
    "\n",
    "유의사항\n",
    "성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, 피처엔지니어링, 분류알고리즘, 하이퍼파라미터 튜닝, 모형 앙상블 등이 수반되어야 한다.\n",
    "수험번호.csv파일이 만들어지도록 코드를 제출한다.\n",
    "\n",
    "\n",
    "채점기준은 정확도로 판단하며 답안 제출은 확률로 하시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 11), (179, 11), (712, 2), (179, 2))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\titanic\\\\train.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='Survived', id_name='PassengerId')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "해당문제 평가지표 0.8309859154929577\n",
      "0.8100806451612902\n",
      "0.9523809523809523\n",
      "0.6451612903225806\n",
      "0.7692307692307692\n",
      "학습 데이터셋의 점수(시험장은 확인불가) 0.9831460674157303\n",
      "실제 평가지표의 점수(시험장은 확인불가) 0.7653631284916201\n",
      "1.0\n",
      "학습 데이터셋의 점수(시험장은 확인불가) 1.0\n",
      "실제 평가지표의 점수(시험장은 확인불가) 0.7653631284916201\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "\n",
    "X_all = X_all.drop( [\"PassengerId\"],axis= 1)\n",
    "\n",
    "# object 변수들에만 na값이 있어서 해당 방식으로 하였다..\n",
    "# X_all = X_all.drop([\"Age\",\"Cabin\"],axis= 1) # 옵션 1\n",
    "X_all.loc[X_all[\"Age\"].isna() == True,\"Age\"] = \"False\" # 옵션 2\n",
    "X_all.loc[X_all[\"Cabin\"].isna() == True,\"Cabin\"] = \"False\" # 옵션 2\n",
    "\n",
    "# print(X_all.info())\n",
    "# print(X_all[\"Age\"].value_counts())\n",
    "# print(X_all[\"Cabin\"].value_counts())\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "# print(col)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Sclaer= StandardScaler()\n",
    "X_all_2 = Sclaer.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = col)\n",
    "# print(X_all_2)\n",
    "x_train = X_all_2[:tr_len]\n",
    "x_test = X_all_2[tr_len:]\n",
    "\n",
    "a = int(np.floor(tr_len/10))\n",
    "a = tr_len - a\n",
    "\n",
    "x_train_1 = x_train[:a]\n",
    "x_val_1 = x_train[a:]\n",
    "\n",
    "# print(x_train_1.index)\n",
    "# print(x_val_1.index)\n",
    "\n",
    "y_train_1 = y_train[:a]\n",
    "y_val_1 = y_train[a:]\n",
    "\n",
    "#여기서 모델 선정\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train_1,y_train_1[\"Survived\"]) # 벨류셋 가능\n",
    "print(model.score(x_train_1,y_train_1[\"Survived\"]))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"predict\":pred,\"ID\":X_test.PassengerId}).to_csv(\"수험번호.csv\",index = False)\n",
    "\n",
    "import sklearn.metrics\n",
    "pred = model.predict(x_val_1)\n",
    "\n",
    "print(\"해당문제 평가지표\",sklearn.metrics.accuracy_score(y_val_1[\"Survived\"],pred)) #정확도\n",
    "print(sklearn.metrics.roc_auc_score(y_val_1[\"Survived\"],pred)) #roc\n",
    "print(sklearn.metrics.precision_score(y_val_1[\"Survived\"],pred)) #정밀도\n",
    "print(sklearn.metrics.recall_score(y_val_1[\"Survived\"],pred)) #재현율\n",
    "print(sklearn.metrics.f1_score(y_val_1[\"Survived\"],pred)) #f1점수\n",
    "print(\"학습 데이터셋의 점수(시험장은 확인불가)\",sklearn.metrics.accuracy_score(y_train[\"Survived\"],model.predict(x_train))) #정확도\n",
    "print(\"실제 평가지표의 점수(시험장은 확인불가)\",sklearn.metrics.accuracy_score(y_test[\"Survived\"],model.predict(x_test))) #정확도\n",
    "\n",
    "# 최종 모델 선정 및 제출\n",
    "from xgboost import XGBClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train,y_train[\"Survived\"]) # 벨류셋 가능\n",
    "print(model.score(x_train,y_train[\"Survived\"]))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"predict\":pred,\"ID\":X_test.PassengerId}).to_csv(\"수험번호.csv\",index = False)\n",
    "print(\"학습 데이터셋의 점수(시험장은 확인불가)\",sklearn.metrics.accuracy_score(y_train[\"Survived\"],model.predict(x_train))) #정확도\n",
    "print(\"실제 평가지표의 점수(시험장은 확인불가)\",sklearn.metrics.accuracy_score(y_test[\"Survived\"],model.predict(x_test))) #정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "x_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_all = X_all.drop(['PassengerId','Cabin',\"Name\",\"Ticket\"]  , axis= 1)\n",
    "X_all = X_all.drop(['Age']  , axis= 1)\n",
    "\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "pd.DataFrame(X_all_2, columns = X_all_1.columns)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler #StandardScaler\n",
    "Scaler = MinMaxScaler()\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = col)\n",
    "\n",
    "x_train = X_all_2[:x_len]\n",
    "x_test = X_all_2[x_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8637640449438202\n",
      "0.7597765363128491\n",
      "0.7999999999999999\n",
      "0.656\n",
      "0.9297752808988764\n",
      "0.7821229050279329\n",
      "0.9045801526717557\n",
      "0.7153284671532847\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "model = XGBRFClassifier(eval_metric = 'logloss',use_label_encoder=False)\n",
    "model.fit(x_train,y_train[['Survived']])\n",
    "pred = model.predict(x_test)\n",
    "print(model.score(x_train,y_train[['Survived']]))\n",
    "print(model.score(x_test,y_test[['Survived']]))\n",
    "print(sklearn.metrics.f1_score(y_train[['Survived']] , model.predict(x_train))) # 학습데이터 f1스코어\n",
    "print(sklearn.metrics.f1_score(y_test[['Survived']], model.predict(x_test))) # 학습데이터 f1스코어\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(eval_metric = 'logloss',use_label_encoder=False)\n",
    "model.fit(x_train,y_train[['Survived']])\n",
    "pred = model.predict(x_test)\n",
    "print(model.score(x_train,y_train[['Survived']]))\n",
    "print(model.score(x_test,y_test[['Survived']]))\n",
    "print(sklearn.metrics.f1_score(y_train[['Survived']] , model.predict(x_train))) # 학습데이터 f1스코어\n",
    "print(sklearn.metrics.f1_score(y_test[['Survived']], model.predict(x_test))) # 학습데이터 f1스코어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8637640449438202\n",
      "0.7597765363128491\n",
      "     pred\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "..    ...\n",
      "174     0\n",
      "175     1\n",
      "176     1\n",
      "177     0\n",
      "178     0\n",
      "\n",
      "[179 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "model = XGBRFClassifier(eval_metric = 'logloss',use_label_encoder=False)\n",
    "model.fit(x_train,y_train[['Survived']])\n",
    "print(model.score(x_train,y_train[['Survived']]))\n",
    "print(model.score(x_test,y_test[['Survived']]))\n",
    "pred = model.predict(x_test)\n",
    "answer = pd.DataFrame({\"pred\":pred})\n",
    "print(answer)\n",
    "answer.to_csv(\"수험번호.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9297752808988764\n",
      "0.7821229050279329\n",
      "0.9045801526717557\n",
      "0.7153284671532847\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(eval_metric = 'logloss',use_label_encoder=False)\n",
    "model.fit(x_train,y_train[['Survived']])\n",
    "pred = model.predict(x_test)\n",
    "print(model.score(x_train,y_train[['Survived']]))\n",
    "print(model.score(x_test,y_test[['Survived']]))\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_train[['Survived']] , model.predict(x_train))) # 학습데이터 f1스코어\n",
    "print(sklearn.metrics.f1_score(y_test[['Survived']], model.predict(x_test))) # 학습데이터 f1스코어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델점수 0.7821229050279329\n",
      "f1_score 0.7153284671532847 재현율 0.6901408450704225 r2_score 0.08959311424100147 혼동행렬 [[[49 22]\n",
      "  [17 91]]\n",
      "\n",
      " [[91 17]\n",
      "  [22 49]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7663667188315075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"모델점수\",model.score(x_test,y_test['Survived']))\n",
    "aa , bb = y_test['Survived'],pred\n",
    "import sklearn.metrics\n",
    "\n",
    "print(\"f1_score\",sklearn.metrics.f1_score(aa , bb), \\\n",
    "\"재현율\",sklearn.metrics.recall_score(aa , bb), \\\n",
    "\"r2_score\",sklearn.metrics.r2_score(aa , bb), \\\n",
    "\"혼동행렬\",sklearn.metrics.multilabel_confusion_matrix(aa , bb))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sklearn.metrics.roc_auc_score(aa , bb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.필요한 컬럼만 뽑음\n",
    "2.더미화\n",
    "3.필요한 컬럼들 정규화.\n",
    "4.예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9789325842696629\n",
      "0.980941475826972\n",
      "0.776536312849162\n",
      "0.7780058651026392\n"
     ]
    }
   ],
   "source": [
    "## 예전 내 정답\n",
    "import pandas as pd\n",
    "df = pd.read_csv(path+ \"\\\\titanic\\\\train.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n",
    "y_train = X_train[['PassengerId', 'Survived']]\n",
    "X_train = X_train.drop(columns=['PassengerId', 'Survived'])\n",
    "y_test = X_test[['PassengerId', 'Survived']]\n",
    "X_test = X_test.drop(columns=['PassengerId', 'Survived'])\n",
    "\n",
    "\n",
    "X_train_len = len(X_train)\n",
    "# print(y_train)\n",
    "\n",
    "X_all = pd.concat([X_train,X_test],axis= 0)\n",
    "# print(y_test.head())\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "###시간남으면 하기####\n",
    "\n",
    "X_train_1 = X_all_1[:X_train_len]\n",
    "X_test_1 = X_all_1[X_train_len:]\n",
    "train_all = pd.concat([X_train_1,y_train['Survived']],axis = 1)\n",
    "train_all_1 = pd.DataFrame(train_all.corr()['Survived'])\n",
    "# print(train_all_1[abs(train_all_1['Survived'])>0.01][:-1].index)\n",
    "Useing_col = list(train_all_1[abs(train_all_1['Survived'])>0.02][:-1].index)\n",
    "# print(len(Useing_col))\n",
    "X_train_1 = X_train_1[Useing_col]\n",
    "X_test_1 = X_test_1[Useing_col]\n",
    "X_all_1 = pd.concat([X_train_1,X_test_1],axis = 0)\n",
    "# print(X_all_1.info())\n",
    "# ###시간남으면 하기####\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_1)\n",
    "\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "# print(X_all_1.shape)\n",
    "# print(y_train['Survived'].shape)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(eval_metric =  \"logloss\",use_label_encoder = False)\n",
    "model.fit(X_train_2,y_train['Survived'])\n",
    "print(model.score(X_train_2,y_train['Survived']))\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "\n",
    "pd.DataFrame({'PassengerId':y_test.PassengerId,'pred or prob':pred}).to_csv(\"수험번호.csv\",index = False)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(model.predict(X_train_2),y_train['Survived']))\n",
    "# print(dir(sklearn.metrics))\n",
    "\n",
    "##여기는 수험자가 확인 불가능 영역\n",
    "print(model.score(X_test_2,y_test['Survived']))\n",
    "print(roc_auc_score(pred,y_test['Survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-2. Pima Indians Diabetes(피마 인디언 당뇨병)\n",
    "#점수 계산은 roc-auc로 판정하며 , prob로 내시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 9), (154, 9), (614, 2), (154, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\archive\\\\diabetes.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='Outcome')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(553, 8) (61, 8) (553, 2) (61, 2)\n",
      "[19:49:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "검증셋 설명력 0.6885245901639344\n",
      "roc_auc_score 트레인셋 0.8852258852258851\n",
      "roc 0.6375291375291375\n",
      "f1 0.5128205128205129\n",
      "실제론 못봄 0.7106167480557062\n",
      "실제론 못봄"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.6296296296296297\n",
      "[19:49:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "트레인셋 설명력 0.8957654723127035\n",
      "roc_auc_score 트레인셋 0.8788235155763057\n",
      "실제론 못봄"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.7018448182311449\n",
      "실제론 못봄 0.616822429906542\n"
     ]
    }
   ],
   "source": [
    "#점수 계산은 roc-auc로 판정하며 , prob로 내시오\n",
    "\n",
    "x_tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train, X_test],axis = 0)\n",
    "X_all = X_all.drop([\"id\"],axis = 1) # id 컬럼은 무쓸모 일거 같아서 드롭.\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "Scaler = MinMaxScaler()\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2, columns = col)\n",
    "\n",
    "\n",
    "# X_all_2 = X_all_1 # 랜포의 경우 사용.  스케일링 안하는 경우가 더 좋을떄도 많음\n",
    "# print(X_all_2)\n",
    "\n",
    "x_train = X_all_2[:x_tr_len]\n",
    "x_test = X_all_2[x_tr_len:]\n",
    "\n",
    "### 평가지표에 따른 모델 선정\n",
    "a = int(np.floor(x_tr_len/10)) * 1\n",
    "a = x_tr_len - a\n",
    "# print(a,x_tr_len)\n",
    "x_train_1 = x_train[:a]\n",
    "x_val_1 = x_train[a:]\n",
    "y_train_1 = y_train[:a]\n",
    "y_val_1 = y_train[a:]\n",
    "print(x_train_1.shape,x_val_1.shape,y_train_1.shape,y_val_1.shape)\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "model = XGBRFClassifier()\n",
    "model.fit(x_train_1,y_train_1[\"Outcome\"])\n",
    "\n",
    "pred = model.predict(x_val_1)\n",
    "prob = model.predict_proba(x_val_1)\n",
    "print(\"검증셋 설명력\",model.score(x_val_1,y_val_1[\"Outcome\"]))\n",
    "\n",
    "from sklearn.metrics import recall_score, roc_auc_score, f1_score, r2_score, precision_score, accuracy_score\n",
    "print(\"roc_auc_score 트레인셋\",roc_auc_score(y_train_1[\"Outcome\"],model.predict(x_train_1)))\n",
    "\n",
    "# print(\"재현율\",recall_score(y_val_1[\"Outcome\"],pred))\n",
    "print(\"roc\",roc_auc_score(y_val_1[\"Outcome\"],pred))\n",
    "print(\"f1\",f1_score(y_val_1[\"Outcome\"],pred))\n",
    "# print(\"r2(설명력)0~1\",r2_score(y_val_1[\"Outcome\"],pred))\n",
    "# print(\"정밀도\",precision_score(y_val_1[\"Outcome\"],pred))\n",
    "# print(\"정확도\",accuracy_score(y_val_1[\"Outcome\"],pred))\n",
    "\n",
    "# print(\"실제론 못봄\",model.score(x_test ,y_test[\"Outcome\"]))\n",
    "print(\"실제론 못봄\",roc_auc_score(y_test[\"Outcome\"],model.predict(x_test)))\n",
    "print(\"실제론 못봄\",f1_score(y_test[\"Outcome\"],model.predict(x_test)))\n",
    "\n",
    "# print(help(sklearn))\n",
    "\n",
    "# 최종 모델 선정\n",
    "from xgboost import XGBRFClassifier\n",
    "model = XGBRFClassifier()\n",
    "model.fit(x_train,y_train[\"Outcome\"])\n",
    "print(\"트레인셋 설명력\",model.score(x_train,y_train[\"Outcome\"]))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "prob = model.predict_proba(x_test)\n",
    "\n",
    "df = pd.DataFrame({\"pred\":pred,\"prob\":prob[:,1],\"id\":X_test.id})\n",
    "df.to_csv(\"수험번호.csv\",index = False)\n",
    "# df = pd.read_csv(\"수험번호.csv\")\n",
    "\n",
    "print(\"roc_auc_score 트레인셋\",roc_auc_score(y_train[\"Outcome\"],model.predict(x_train)))\n",
    "\n",
    "# print(\"실제론 못봄\",model.score(x_test ,y_test[\"Outcome\"]))\n",
    "print(\"실제론 못봄\",roc_auc_score(y_test[\"Outcome\"],model.predict(x_test)))\n",
    "print(\"실제론 못봄\",f1_score(y_test[\"Outcome\"],model.predict(x_test)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRFClassifier\n",
    "검증셋 설명력 0.6229508196721312\n",
    "roc_auc_score 트레인셋 1.0\n",
    "roc 0.596153846153846\n",
    "f1 0.4888888888888889\n",
    "실제론 못봄 0.7250859106529208\n",
    "실제론 못봄 0.6551724137931034\n",
    "\n",
    "XGBClassifier\n",
    "검증셋 설명력 0.6229508196721312\n",
    "roc 0.596153846153846\n",
    "f1 0.4888888888888889\n",
    "실제론 못봄 0.7250859106529208\n",
    "실제론 못봄 0.6551724137931034\n",
    "\n",
    "GradientBoostingClassifier\n",
    "검증셋 설명력 0.6557377049180327\n",
    "roc_auc_score 트레인셋 0.9175824175824175\n",
    "roc 0.6118881118881119\n",
    "f1 0.4878048780487805\n",
    "실제론 못봄 0.6982275275818413\n",
    "실제론 못봄 0.6095238095238096\n",
    "\n",
    "RandomForestClassifier (스케일링 x)\n",
    "검증셋 설명력 0.6557377049180327\n",
    "roc_auc_score 트레인셋 1.0\n",
    "roc 0.6118881118881119\n",
    "f1 0.4878048780487805\n",
    "실제론 못봄 0.7296979562307832\n",
    "실제론 못봄 0.6542056074766356\n",
    "\n",
    "검증셋 설명력 0.6885245901639344\n",
    "roc_auc_score 트레인셋 1.0\n",
    "roc 0.6474358974358976\n",
    "f1 0.5365853658536586\n",
    "실제론 못봄 0.7503165129318141\n",
    "실제론 못봄 0.679611650485437\n",
    "\n",
    "XGBRFClassifier(스케일링x)\n",
    "검증셋 설명력 0.7049180327868853\n",
    "roc_auc_score 트레인셋 0.891992266992267\n",
    "roc 0.6404428904428905\n",
    "f1 0.5000000000000001\n",
    "실제론 못봄 0.6966901790558871\n",
    "실제론 못봄 0.6111111111111113\n",
    "\n",
    "!! 트레인셋이 1.0은 과적합 모델로 판단하자!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.7597402597402597\n",
      "0.6407766990291262\n"
     ]
    }
   ],
   "source": [
    "x_tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test], axis= 0)\n",
    "# X_all.info() # 널값 없음\n",
    "X_all_1 = X_all.drop([\"id\"],axis = 1)\n",
    "X_all_1 = pd.get_dummies(X_all_1)\n",
    "col = X_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Sclaer = MinMaxScaler()\n",
    "X_all_2 = Sclaer.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = col)\n",
    "\n",
    "x_train = X_all_2[:x_tr_len]\n",
    "x_test = X_all_2[x_tr_len:]\n",
    "\n",
    "# y_train = y_train.reset_index()\n",
    "# train_all = pd.concat([x_train,y_train], axis = 1)\n",
    "# del train_all[\"id\"]\n",
    "# train_all.corr()[\"Outcome\"][:-1]\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(x_train ,y_train[\"Outcome\"])\n",
    "print(model.score(x_train ,y_train[\"Outcome\"]))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"preidct\":pred}).to_csv(\"수험번호.csv\", index =False)\n",
    "# pd.read_csv(\"수험번호.csv\")\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.f1_score(y_train[\"Outcome\"],model.predict(x_train)))\n",
    "\n",
    "\n",
    "print(model.score(x_test ,y_test[\"Outcome\"]))\n",
    "print(sklearn.metrics.f1_score(y_test[\"Outcome\"],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8957654723127035\n",
      "0.8446601941747572\n",
      "0.7337662337662337\n",
      "0.616822429906542\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = XGBRFClassifier()\n",
    "model.fit(x_train ,y_train[\"Outcome\"])\n",
    "print(model.score(x_train ,y_train[\"Outcome\"]))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"preidct\":pred}).to_csv(\"수험번호.csv\", index =False)\n",
    "# pd.read_csv(\"수험번호.csv\")\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.f1_score(y_train[\"Outcome\"],model.predict(x_train)))\n",
    "\n",
    "\n",
    "print(model.score(x_test ,y_test[\"Outcome\"]))\n",
    "print(sklearn.metrics.f1_score(y_test[\"Outcome\"],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_train ,y_train[\"Outcome\"])\n",
    "print(model.score(x_train ,y_train[\"Outcome\"]))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"preidct\":pred}).to_csv(\"수험번호.csv\", index =False)\n",
    "# pd.read_csv(\"수험번호.csv\")\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.f1_score(y_train[\"Outcome\"],model.predict(x_train)))\n",
    "\n",
    "\n",
    "\n",
    "print(model.score(x_test ,y_test[\"Outcome\"]))\n",
    "print(sklearn.metrics.f1_score(y_test[\"Outcome\"],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = SVC()\n",
    "model.fit(x_train ,y_train[\"Outcome\"])\n",
    "print(model.score(x_train ,y_train[\"Outcome\"]))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"preidct\":pred}).to_csv(\"수험번호.csv\", index =False)\n",
    "# pd.read_csv(\"수험번호.csv\")\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.f1_score(y_train[\"Outcome\"],model.predict(x_train)))\n",
    "\n",
    "\n",
    "\n",
    "print(model.score(x_test ,y_test[\"Outcome\"]))\n",
    "print(sklearn.metrics.f1_score(y_test[\"Outcome\"],pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-3. Adult Census Income Tutorial\n",
    "age: 나이\n",
    "workclass: 고용 형태\n",
    "fnlwgt: 사람의 대표성을 나타내는 가중치(final weight)\n",
    "education: 교육 수준\n",
    "education.num: 교육 수준 수치\n",
    "marital.status: 결혼 상태\n",
    "occupation: 업종\n",
    "relationship: 가족 관계\n",
    "race: 인종\n",
    "sex: 성별\n",
    "capital.gain: 양도 소득\n",
    "capital.loss: 양도 손실\n",
    "hours.per.week: 주당 근무 시간\n",
    "native.country: 국적\n",
    "income: 수익 (예측해야 하는 값)\n",
    "\n",
    "채점기준은 정밀도로 판단하며 답안 제출은 확률로 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26048, 15), (6513, 15), (26048, 2), (6513, 2))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\archive\\\\adult.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='income', null_name='?')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Happy\\AppData\\Local\\Temp\\ipykernel_20612\\1745761693.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train[\"income\"] = LE.fit_transform(y_train[\"income\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 설명력 0.8700733663197406\n",
      "정밀도 0.7913223140495868\n",
      "실제 정밀도(알수 x) 0.7888982338099243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Happy\\AppData\\Local\\Temp\\ipykernel_20612\\1745761693.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_test[\"income\"] = LE.transform(y_test[\"income\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "\n",
    "# print(X_all.describe())\n",
    "\n",
    "# print(X_all[\"workclass\"].value_counts())\n",
    "# print(X_all[\"occupation\"].value_counts())\n",
    "# print(X_all[\"native.country\"].value_counts())\n",
    "\n",
    "#명목형 변수의 경우에 있는 na값들은 의미가 있을 수 있다고 생각하여 False로 na값들을 바꾸어 더미화 전에 새로운 파생변수가 되게 하였다.\n",
    "X_all.loc[X_all[\"workclass\"].isna() == True,'workclass'] = \"False\"\n",
    "X_all.loc[X_all[\"occupation\"].isna() == True,'occupation'] = \"False\"\n",
    "X_all.loc[X_all[\"native.country\"].isna() == True,\"native.country\"] = \"False\"\n",
    "X_all = X_all.drop([\"id\"],axis= 1)\n",
    "# print(X_all.info())\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelEncoder\n",
    "Scaler = MinMaxScaler()\n",
    "\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2, columns = col)\n",
    "\n",
    "# X_all_2 = X_all_1 # @ 의사결정 나무일 경우만 (의사결정 나무는 분리하기 때문에 범위가 넓으면 오히려 좋을수도 있다.)\n",
    "\n",
    "x_train = X_all_2[:x_tr_len]\n",
    "x_test = X_all_2[x_tr_len:]\n",
    "\n",
    "LE = LabelEncoder()\n",
    "y_train[\"income\"] = LE.fit_transform(y_train[\"income\"])\n",
    "\n",
    "# ## 모델 선정 하기 \n",
    "a = int(np.floor(x_tr_len/10))\n",
    "a = x_tr_len - a\n",
    "x_train_1 = x_train[:a]\n",
    "x_val_1 = x_train[a:]\n",
    "# print(x_train_1.index)\n",
    "# print(x_val_1.index)\n",
    "y_train_1 = y_train[:a]\n",
    "y_val_1 = y_train[a:]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(x_train_1,y_train_1[\"income\"])\n",
    "pred = model.predict(x_val_1)\n",
    "# prob = model.predict_proba(x_val_1)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(\"모델 설명력\",model.score(x_train_1,y_train_1[\"income\"]))\n",
    "print(\"정밀도\",precision_score(y_val_1[\"income\"],pred))\n",
    "\n",
    "y_test[\"income\"] = LE.transform(y_test[\"income\"])\n",
    "print(\"실제 정밀도(알수 x)\",precision_score(y_test[\"income\"],model.predict(x_test)))\n",
    "\n",
    "\n",
    "# ## 제출용 모델 \n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from xgboost import XGBRFClassifier\n",
    "# model = XGBRFClassifier()\n",
    "# model.fit(x_train,y_train[\"income\"])\n",
    "# pred = model.predict(x_test)\n",
    "# prob = model.predict_proba(x_test)\n",
    "# pred = LE.inverse_transform(pred) # 라벨인코더 역변환.\n",
    "\n",
    "# pd.DataFrame({\"pred\":pred,\"prob\":prob[:,1],\"id\":X_test.id}).to_csv(\"수험번호.csv\",index = False)\n",
    "\n",
    "# y_test[\"income\"] = LE.transform(y_test[\"income\"])# 실제 정밀도 보려고\n",
    "# print(\"실제 정밀도(알수 x)\",precision_score(y_test[\"income\"],model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "X_all = X_all.drop([\"id\"],axis = 1)\n",
    "# na값이 있는 행들 넣고 했는데 오히려 좋았다..\n",
    "# X_all = X_all.drop([\"id\",\"workclass\",\"occupation\",\"native.country\"],axis = 1) \n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Sclaer =  MinMaxScaler()\n",
    "X_all_2 = Sclaer.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = col)\n",
    "\n",
    "x_train = X_all_2[:tr_len]\n",
    "x_test = X_all_2[tr_len:]\n",
    "\n",
    "x_train_1 = x_train[:-2604]\n",
    "x_val = x_train[-2604:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_all = pd.concat([y_train,y_test],axis = 0)\n",
    "\n",
    "# Y_all.reset_index(drop = True)\n",
    "\n",
    "# Y_all.loc[Y_all[\"income\"]== \"<=50K\",\"income\"] = 1.0\n",
    "# Y_all.loc[Y_all[\"income\"]== \">50K\",\"income\"] = 0.0\n",
    "\n",
    "\n",
    "import sklearn.preprocessing\n",
    "LE = sklearn.preprocessing.LabelEncoder()\n",
    "Y_all['income'] = LE.fit_transform(Y_all['income'])\n",
    "\n",
    "y_train = Y_all[:tr_len]\n",
    "y_test = Y_all[tr_len:]\n",
    "\n",
    "y_train_1 = y_train[:-2604]\n",
    "y_val = y_train[-2604:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관분석이 악영향을 끼침..\n",
    "# train_all = pd.concat([x_train,y_train],axis = 1)\n",
    "# corr = pd.DataFrame(train_all.corr()[\"income\"][:-1])\n",
    "# corr = corr.drop([\"id\"])\n",
    "# corr_col = corr.loc[abs(corr[\"income\"])>0.01].index\n",
    "\n",
    "# x_train = x_train[corr_col]\n",
    "# x_test = x_test[corr_col]\n",
    "# x_train_1 = x_train_1[corr_col]\n",
    "# x_val = x_val[corr_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()# 이진 분류 모델\n",
    "# model = XGBClassifier(eval_metric = \"logloss\" )\n",
    "model.fit(x_train ,y_train[\"income\"])\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "pd.DataFrame({\"preidict\":pred,\"id\":y_test.id}).to_csv(\"수험번호.csv\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9035626535626535\n",
      "0.9043778801843319\n",
      "0.9043778801843319 0.7855297157622739\n",
      "0.8754798096115461 0.7167307020607754\n"
     ]
    }
   ],
   "source": [
    "print(model.score(x_train ,y_train[\"income\"]))\n",
    "print(model.score(x_val ,y_val[\"income\"]))\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_val[\"income\"],model.predict(x_val)) ,\n",
    "sklearn.metrics.f1_score(y_val[\"income\"],model.predict(x_val))\n",
    ")\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_test[\"income\"],model.predict(x_test)) ,\n",
    "sklearn.metrics.f1_score(y_test[\"income\"],model.predict(x_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9033323095823096\n",
      "0.8725038402457758\n",
      "0.8725038402457758 0.710801393728223\n",
      "0.8725625671733456 0.7087719298245615\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()# 이진 분류 모델\n",
    "# model = XGBClassifier(eval_metric = \"logloss\" )\n",
    "model.fit(x_train_1 ,y_train_1[\"income\"])\n",
    "pred = model.predict(x_val)\n",
    "\n",
    "\n",
    "print(model.score(x_train ,y_train[\"income\"]))\n",
    "print(model.score(x_val ,y_val[\"income\"]))\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_val[\"income\"],model.predict(x_val)) ,\n",
    "sklearn.metrics.f1_score(y_val[\"income\"],model.predict(x_val))\n",
    ")\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_test[\"income\"],model.predict(x_test)) ,\n",
    "sklearn.metrics.f1_score(y_test[\"income\"],model.predict(x_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9862177518427518\n",
      "0.8621351766513057\n",
      "0.8621351766513057 0.6897147796024201\n",
      "0.8526024873330262 0.6668979875086745\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()# 이진 분류 모델\n",
    "# model = XGBClassifier(eval_metric = \"logloss\" )\n",
    "model.fit(x_train_1 ,y_train_1[\"income\"])\n",
    "pred = model.predict(x_val)\n",
    "\n",
    "print(model.score(x_train ,y_train[\"income\"]))\n",
    "print(model.score(x_val ,y_val[\"income\"]))\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_val[\"income\"],model.predict(x_val)) ,\n",
    "sklearn.metrics.f1_score(y_val[\"income\"],model.predict(x_val))\n",
    ")\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_test[\"income\"],model.predict(x_test)) ,\n",
    "sklearn.metrics.f1_score(y_test[\"income\"],model.predict(x_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8704315110565111\n",
      "0.8709677419354839\n",
      "0.8709677419354839 0.6945454545454546\n",
      "0.868417012129587 0.6861955327718784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier()# 이진 분류 모델\n",
    "# model = XGBClassifier(eval_metric = \"logloss\" )\n",
    "model.fit(x_train_1 ,y_train_1[\"income\"])\n",
    "pred = model.predict(x_val)\n",
    "\n",
    "print(model.score(x_train ,y_train[\"income\"]))\n",
    "print(model.score(x_val ,y_val[\"income\"]))\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_val[\"income\"],model.predict(x_val)) ,\n",
    "sklearn.metrics.f1_score(y_val[\"income\"],model.predict(x_val))\n",
    ")\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_test[\"income\"],model.predict(x_test)) ,\n",
    "sklearn.metrics.f1_score(y_test[\"income\"],model.predict(x_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8603347665847666\n",
      "0.8667434715821812\n",
      "0.8667434715821812 0.691006233303651\n",
      "0.861200675571933 0.6764495347172512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "model = AdaBoostClassifier()# 이진 분류 모델\n",
    "# model = XGBClassifier(eval_metric = \"logloss\" )\n",
    "model.fit(x_train_1 ,y_train_1[\"income\"])\n",
    "pred = model.predict(x_val)\n",
    "\n",
    "print(model.score(x_train ,y_train[\"income\"]))\n",
    "print(model.score(x_val ,y_val[\"income\"]))\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_val[\"income\"],model.predict(x_val)) ,\n",
    "sklearn.metrics.f1_score(y_val[\"income\"],model.predict(x_val))\n",
    ")\n",
    "\n",
    "print(\n",
    "sklearn.metrics.accuracy_score(y_test[\"income\"],model.predict(x_test)) ,\n",
    "sklearn.metrics.f1_score(y_test[\"income\"],model.predict(x_test))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-4. House Prices (Regression)\n",
    "RMSE 로 평가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[id_name, target])\n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[id_name, target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\house-prices-advanced-regression-techniques\\\\train.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='SalePrice', id_name='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 설명력(트레인) 0.9480177097880658\n",
      "모델 설명력(val) 0.8468798045980142\n",
      "0.9697842648569498\n",
      "모델 MSE(실제값)(시험장확인불가) 559087698.0904062\n"
     ]
    }
   ],
   "source": [
    "X_tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "# object 데이터의 na값을 변환.. 중\n",
    "col = X_all.select_dtypes(\"object\").columns\n",
    "for i in range(len(col)):\n",
    "    if(X_all[col[i]].isna().sum() == 0):\n",
    "        pass\n",
    "    else:\n",
    "        X_all.loc[X_all[col[i]].isna() == True,col[i]] = \"False\"\n",
    "        \n",
    "        \n",
    "# int64 데이터 na값을 변환.. 중\n",
    "col = X_all.select_dtypes(\"int64\").columns\n",
    "for i in range(len(col)):\n",
    "    if(X_all[col[i]].isna().sum() == 0):\n",
    "        pass\n",
    "    else:\n",
    "        X_all.loc[X_all[col[i]].isna() == True,col[i]] = 0\n",
    "        \n",
    "\n",
    "# float64 데이터 na값을 변환.. 중\n",
    "col = X_all.select_dtypes(\"float64\").columns\n",
    "for i in range(len(col)):\n",
    "    if(X_all[col[i]].isna().sum() == 0):\n",
    "        pass\n",
    "    else:\n",
    "        X_all.loc[X_all[col[i]].isna() == True,col[i]] = 0\n",
    "        \n",
    "        \n",
    "# print(X_all.info())\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "cols = X_all_1.columns\n",
    "# print(X_all_1.info())\n",
    "# print(X_all_1.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "Sclaer = MinMaxScaler()\n",
    "X_all_2 = Sclaer.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = cols)\n",
    "\n",
    "\n",
    "\n",
    "x_train = X_all_2[:X_tr_len]\n",
    "x_test = X_all_2[X_tr_len:]\n",
    "\n",
    "# y_train[\"SalePrice\"]\n",
    "\n",
    "# a = int(np.floor(X_tr_len/10)) * 2\n",
    "# a = X_tr_len - a\n",
    "# # print(a,X_tr_len)\n",
    "# x_train_1 = x_train[:a]\n",
    "# x_val_1 = x_train[a:]\n",
    "# # print(len(x_train_1),len(x_val_1))\n",
    "\n",
    "# y_train_1 = y_train[:a]\n",
    "# y_val_1 =y_train[a:]\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train_1,x_val_1,y_train_1,y_val_1 = train_test_split(x_train,y_train, random_state = 2022, train_size= 0.8)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "# from xgboost import XGBRegressor,XGBRFRegressor\n",
    "# model = XGBRFRegressor()\n",
    "# model.fit(x_train_1,y_train_1[\"SalePrice\"])\n",
    "# print(\"모델 설명력(트레인)\",model.score(x_train_1,y_train_1[\"SalePrice\"]))\n",
    "\n",
    "# print(\"모델 설명력(val)\",model.score(x_val_1,y_val_1[\"SalePrice\"]))\n",
    "\n",
    "# pred = model.predict(x_val_1)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# print(\"모델 MSE\",mean_squared_error(y_val_1[\"SalePrice\"],pred))\n",
    "\n",
    "# print(\"모델 MSE(실제값)(시험장확인불가)\",mean_squared_error(y_test[\"SalePrice\"],model.predict(x_test)))\n",
    "# min([1767763024.0674014,1140030387.9762945,1088415594.122977,1243247979.6752555,1339693846.7893748])\n",
    "# min([1065719644.2969407,685151071.1174757,551757163.8349265,794757624.5194604,837418665.4934312])\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor,XGBRFRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(x_train,y_train[\"SalePrice\"])\n",
    "print(model.score(x_train,y_train[\"SalePrice\"]))\n",
    "pred = model.predict(x_test)\n",
    "df= pd.DataFrame({\"predict\":pred}).reset_index(drop = True)\n",
    "df.to_csv(\"수험번호.csv\",index = False)\n",
    "\n",
    "print(\"모델 MSE(실제값)(시험장확인불가)\",mean_squared_error(y_test[\"SalePrice\"],model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoostRegressor\n",
    "모델 설명력(트레인) 0.8790075442855299\n",
    "모델 설명력(val) 0.6142381256426341\n",
    "모델 MSE 1767763024.0674014\n",
    "모델 MSE(실제값) 1065719644.2969407\n",
    "\n",
    "RandomForestRegressor\n",
    "모델 설명력(트레인) 0.9809314035809918\n",
    "모델 설명력(val) 0.7512221642252642\n",
    "모델 MSE 1140030387.9762945\n",
    "모델 MSE(실제값) 685151071.1174757\n",
    "\n",
    "GradientBoostingRegressor\n",
    "모델 설명력(트레인) 0.9759104438467778\n",
    "모델 설명력(val) 0.7624855628541212\n",
    "모델 MSE 1088415594.122977\n",
    "모델 MSE(실제값) 551757163.8349265\n",
    "\n",
    "XGBRegressor\n",
    "모델 설명력(트레인) 0.99989189501608\n",
    "모델 설명력(val) 0.7286979847406013\n",
    "모델 MSE 1243247979.6752555\n",
    "모델 MSE(실제값) 794757624.5194604\n",
    "\n",
    "XGBRFRegressor\n",
    "모델 설명력(트레인) 0.9543875647411199\n",
    "모델 설명력(val) 0.7076515333976154\n",
    "모델 MSE 1339693846.7893748\n",
    "모델 MSE(실제값) 837418665.4934312\n",
    "\n",
    "트레인 설명력이 너무 높은 xgboost는 빼고.\n",
    "val 설명력이 제일 낮은 ada도 뺸다\n",
    "그리고 mse가 제일 낮은 모델 선택.. ㄱㄷ \n",
    "min([1767763024.0674014,1140030387.9762945,1088415594.122977,1243247979.6752555,1339693846.7893748])\n",
    "=>1088415594.122977\n",
    "\n",
    "=>GradientBoostingRegressor 모델을 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[18:51:06] C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/data/data.cc:592: Check failed: labels_.Size() == num_row_ (936 vs. 934) : Size of labels must equal to number of rows.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-88e45961abc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mXGBRFRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXGBRFRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SalePrice\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"모델 설명력(트레인)\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"SalePrice\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__class__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m         \u001b[0m_check_rf_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    194\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[0;32m    197\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1680\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1681\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1682\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1683\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1684\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \"\"\"\n\u001b[0;32m    217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [18:51:06] C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/data/data.cc:592: Check failed: labels_.Size() == num_row_ (936 vs. 934) : Size of labels must equal to number of rows."
     ]
    }
   ],
   "source": [
    "X_tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "# object 데이터의 na값을 변환.. 중\n",
    "col = X_all.select_dtypes(\"object\").columns\n",
    "for i in range(len(col)):\n",
    "    if(X_all[col[i]].isna().sum() == 0):\n",
    "        pass\n",
    "    else:\n",
    "        X_all.loc[X_all[col[i]].isna() == True,col[i]] = \"False\"\n",
    "        \n",
    "        \n",
    "# int64 데이터 na값을 변환.. 중\n",
    "col = X_all.select_dtypes(\"int64\").columns\n",
    "for i in range(len(col)):\n",
    "    if(X_all[col[i]].isna().sum() == 0):\n",
    "        pass\n",
    "    else:\n",
    "        X_all.loc[X_all[col[i]].isna() == True,col[i]] = 0\n",
    "        \n",
    "\n",
    "# float64 데이터 na값을 변환.. 중\n",
    "col = X_all.select_dtypes(\"float64\").columns\n",
    "for i in range(len(col)):\n",
    "    if(X_all[col[i]].isna().sum() == 0):\n",
    "        pass\n",
    "    else:\n",
    "        X_all.loc[X_all[col[i]].isna() == True,col[i]] = 0\n",
    "        \n",
    "        \n",
    "# print(X_all.info())\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "cols = X_all_1.columns\n",
    "# print(X_all_1.info())\n",
    "# print(X_all_1.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelEncoder\n",
    "Sclaer = MinMaxScaler()\n",
    "X_all_2 = Sclaer.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = cols)\n",
    "\n",
    "\n",
    "\n",
    "x_train = X_all_2[:X_tr_len]\n",
    "x_test = X_all_2[X_tr_len:]\n",
    "\n",
    "# y_train[\"SalePrice\"]\n",
    "\n",
    "# a = int(np.floor(X_tr_len/10)) * 2\n",
    "# a = X_tr_len - a\n",
    "# # print(a,X_tr_len)\n",
    "# x_train_1 = x_train[:a]\n",
    "# x_val_1 = x_train[a:]\n",
    "# # print(len(x_train_1),len(x_val_1))\n",
    "\n",
    "# y_train_1 = y_train[:a]\n",
    "# y_val_1 =y_train[a:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_1,x_val_1,y_trian_1,y_val_1 = train_test_split(x_train,y_train, random_state = 2022, train_size= 0.8)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor,XGBRFRegressor\n",
    "model = XGBRFRegressor()\n",
    "model.fit(x_train_1,y_train_1[\"SalePrice\"])\n",
    "print(\"모델 설명력(트레인)\",model.score(x_train_1,y_train_1[\"SalePrice\"]))\n",
    "\n",
    "print(\"모델 설명력(val)\",model.score(x_val_1,y_val_1[\"SalePrice\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = len(X_train)\n",
    "X_all = pd.concat( [X_train,X_test])\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "X_all_col = X_all_1.columns\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2, columns = X_all_col)\n",
    "\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "\n",
    "y_train_1 = y_train.reset_index(drop= True)\n",
    "y_test_1 = y_test.reset_index(drop= True)\n",
    "\n",
    "Train_all = pd.concat([X_train_2,y_train_1],axis = 1)\n",
    "aa = pd.DataFrame(abs(Train_all.corr()['SalePrice'])>0.1)\n",
    "use_columns = aa.loc[aa['SalePrice']==True][:-1].index\n",
    "\n",
    "X_train_3 = X_train_2[use_columns]\n",
    "X_test_3 = X_test_2[use_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-36c7d7f9e1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SalePrice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mallow_nd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_regressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         )\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m     )\n\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    114\u001b[0m             raise ValueError(\n\u001b[0;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m--> 116\u001b[1;33m                     \u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                 )\n\u001b[0;32m    118\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(X_train_3,y_train_1['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491270042093133\n",
      "0.8502719975650403\n",
      "31124.538902331296\n",
      "29559.060750897082\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(learning_rate = 0.3,booster = \"gblinear\", reg_lambda = 1.2, reg_alpha = 0.2) # @@ gbliner를 이용하자\n",
    "# model = XGBRegressor()\n",
    "model.fit(X_train_3,y_train_1['SalePrice'])\n",
    "\n",
    "print(model.score(X_train_3,y_train_1['SalePrice']))\n",
    "\n",
    "pred = model.predict(X_test_3)\n",
    "\n",
    "pd.DataFrame({\"predict\":pred}).to_csv(\"수험번호.csv\", index = False)\n",
    "# pred\n",
    "\n",
    "print(model.score(X_test_3,y_test_1['SalePrice']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train_1['SalePrice'], model.predict(X_train_3)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test_1['SalePrice'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997592786239734\n",
      "0.9074264332872578\n",
      "1243.2386352266383\n",
      "23242.48788008825\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(learning_rate = 0.1,booster = \"gblinear\") # @@ gbliner를 이용하자\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train_2,y_train_1['SalePrice'])\n",
    "\n",
    "print(model.score(X_train_2,y_train_1['SalePrice']))\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "# pred\n",
    "\n",
    "print(model.score(X_test_2,y_test_1['SalePrice']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train_1['SalePrice'], model.predict(X_train_2)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test_1['SalePrice'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-5. Insurance_Starter (Tutorial)\n",
    "MAE 지표로 측정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070, 7), (268, 7), (1070, 2), (268, 2))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\archive\\\\insurance.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='charges')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "del X_all[\"id\"]\n",
    "# print(X_all.info()) # na값 없고\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "\n",
    "# print(col)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Scaler = MinMaxScaler()\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = col)\n",
    "\n",
    "x_train = X_all_2[:x_tr_len]\n",
    "x_test = X_all_2[x_tr_len:]\n",
    "\n",
    "a = int(np.floor(x_tr_len/10))\n",
    "a = x_tr_len - a\n",
    "x_train_1 = x_train[:a]\n",
    "x_val_1 = x_train[a:]\n",
    "# print(x_train_1,x_val_1)\n",
    "y_train_1 = y_train[:a]\n",
    "y_val_1 = y_train[a:]\n",
    "\n",
    "from sklearn.metrics import median_absolute_error\n",
    "# from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor,GradientBoostingRegressor\n",
    "# from xgboost import XGBRFRegressor,XGBRegressor\n",
    "# model = GradientBoostingRegressor()\n",
    "# model.fit(x_train_1,y_train_1[\"charges\"])\n",
    "# pred = model.predict(x_val_1)\n",
    "\n",
    "# print(\"모델 설명력(트레인셋)\",model.score(x_train_1,y_train_1[\"charges\"]))\n",
    "# print(\"모델 설명력(트레인셋)\",model.score(x_val_1,y_val_1[\"charges\"]))\n",
    "# print(\"MAE 검증셋\",median_absolute_error(y_val_1[\"charges\"],pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 최종모델 선정\n",
    "from sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor,GradientBoostingRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "model = RandomForestRegressor()\n",
    "model.fit(x_train,y_train[\"charges\"])\n",
    "pred = model.predict(x_test)\n",
    "df = pd.DataFrame({\"predict\":pred,\"id\":X_test.id}).reset_index(drop=True)\n",
    "df.to_csv(\"수험번호.csv\",index = False)\n",
    "\n",
    "print(\"MAE 실제셋(실제론 검증 불가)\",median_absolute_error(y_test[\"charges\"],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRFRegressor\n",
    "모델 설명력(트레인셋) 0.9226719114538227\n",
    "모델 설명력(트레인셋) 0.7708086583198435\n",
    "MAE 검증셋 1055.4368984374996\n",
    "\n",
    "XGBRegressor\n",
    "모델 설명력(트레인셋) 0.9975642736229078\n",
    "모델 설명력(트레인셋) 0.717428078652037\n",
    "MAE 검증셋 837.2297000000035\n",
    "\n",
    "AdaBoostRegressor\n",
    "모델 설명력(트레인셋) 0.8333617317393505\n",
    "모델 설명력(트레인셋) 0.7327147902027478\n",
    "MAE 검증셋 3418.1171138489126\n",
    "\n",
    "\n",
    "RandomForestRegressor\n",
    "모델 설명력(트레인셋) 0.9788083746675367\n",
    "모델 설명력(트레인셋) 0.7454812527330513\n",
    "MAE 검증셋 806.6165518000016\n",
    "\n",
    "GradientBoostingRegressor\n",
    "모델 설명력(트레인셋) 0.9172395012755328\n",
    "모델 설명력(트레인셋) 0.7835640895652287\n",
    "MAE 검증셋 956.2292788178711\n",
    "\n",
    "가장 좋은 모델인 랜덤포레스트리그레션 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964181737350261\n",
      "0.8120888367117656\n"
     ]
    }
   ],
   "source": [
    "X_train_len = len(X_train)\n",
    "\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "x_train = X_all_1[:X_train_len]\n",
    "x_test = X_all_1[X_train_len:]\n",
    "\n",
    "train_all = pd.concat([x_train,y_train],axis = 1)\n",
    "df1 = pd.DataFrame(abs(train_all.corr()['charges'])>0.007)\n",
    "train_col = df1.loc[df1['charges']==True].index[:-1]\n",
    "X_all_2 = X_all_1[train_col]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Scaler = MinMaxScaler()\n",
    "X_all_3 = Scaler.fit_transform(X_all_2)\n",
    "X_all_3 = pd.DataFrame(X_all_3,columns = train_col)\n",
    "\n",
    "x_train = X_all_3[:X_train_len]\n",
    "x_test = X_all_3[X_train_len:]\n",
    "# y_train.shape, y_test.shape\n",
    "\n",
    "# y_train = y_train.reset_index(drop=True)\n",
    "# y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(x_train,y_train['charges'])\n",
    "print(model.score(x_train,y_train['charges']))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"predict\":pred}).to_csv(\"수험번호.csv\",index= False)\n",
    "\n",
    "print(model.score(x_test,y_test['charges']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964181737350261\n",
      "0.8120559186498852\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# model = XGBRegressor(learning_rate = 0.1,booster = \"gblinear\") # @@ gbliner를 이용하자??\n",
    "model = XGBRegressor()\n",
    "# model = XGBRegressor(booster = \"gblinear\")\n",
    "model.fit(x_train,y_train['charges'])\n",
    "\n",
    "print(model.score(x_train,y_train['charges']))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "# pred\n",
    "\n",
    "print(model.score(x_test,y_test['charges']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train['charges'], model.predict(x_train)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test['charges'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-6. Bike-sharing-demand (Regression)_2[joge]\n",
    "평가지표 \n",
    "rmsle => 루트 민 스퀘어 로그 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[id_name, target])\n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[id_name, target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\bike\\\\train.csv\")\n",
    "    \n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='count')#, id_name='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/bike-sharing-demand/train.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='count')#, id_name='Id')\n",
    "tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train, X_test],axis = 0)\n",
    "# print(X_all.info())\n",
    "# print(X_all,y_train)\n",
    "\n",
    "#count\n",
    "X_all[\"datetime\"] = pd.to_datetime(X_all[\"datetime\"])\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "del X_all_1[\"datetime\"]\n",
    "col = X_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Scaler = MinMaxScaler()\n",
    "# print(dir(Scaler))\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2,columns = col)\n",
    "\n",
    "x_train = X_all_2[:tr_len]\n",
    "x_test = X_all_2[tr_len:]\n",
    "\n",
    "# # 모델 결정\n",
    "# a = int(np.floor(tr_len/10))\n",
    "# a = tr_len - a\n",
    "\n",
    "# x_train_1 = x_train[:a]\n",
    "# x_val_1 = x_train[a:]\n",
    "# y_train_1 = y_train[:a]\n",
    "# y_val_1 = y_train[a:]\n",
    "# # print(x_train_1)\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostRegressor,BaggingRegressor, ExtraTreesRegressor , GradientBoostingRegressor, RandomForestRegressor\n",
    "# from xgboost import XGBRFRegressor, XGBRegressor\n",
    "# from sklearn.metrics import mean_squared_log_error,\n",
    "# model = RandomForestRegressor()\n",
    "# model.fit(x_train_1,y_train_1[\"count\"])\n",
    "# print(\"학습셋 설명력\",model.score(x_train_1,y_train_1[\"count\"]))\n",
    "# print(\"검증셋 설명력\",model.score(x_val_1,y_val_1[\"count\"]))\n",
    "# pred = model.predict(x_val_1)\n",
    "# print(\"검증셋 msle\",mean_squared_log_error(y_val_1[\"count\"],pred))\n",
    "\n",
    "# min([0.021284678357855375,0.0005045399261525063,0.5078807945567321,0.00020697400787588686,6.005228399762248e-05,0.0037368855560598892,0.0001629502041205388])\n",
    "\n",
    "# 최종모델 선정\n",
    "from sklearn.ensemble import AdaBoostRegressor,BaggingRegressor, ExtraTreesRegressor , GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRFRegressor, XGBRegressor\n",
    "model = ExtraTreesRegressor()\n",
    "model.fit(x_train,y_train[\"count\"])\n",
    "print(model.score(x_train,y_train[\"count\"]))\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "pd.DataFrame({\"predict\":pred}).to_csv(\"수험번호.csv\",index = False)\n",
    "df = pd.DataFrame({\"predict\":pred}).reset_index(drop = True)\n",
    "df.to_csv(\"수험번호.csv\",index = False) # \n",
    "print(pd.read_csv(\"수험번호.csv\"))\n",
    "\n",
    "print(\"실제셋 msle\",mean_squared_log_error(y_test[\"count\"],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRFRegressor\n",
    "학습셋 설명력 0.9979970941248987\n",
    "검증셋 설명력 0.9970463105653579\n",
    "검증셋 msle 0.021284678357855375\n",
    "\n",
    "XGBRegressor\n",
    "학습셋 설명력 0.9999401287245397\n",
    "검증셋 설명력 0.9994642454729618\n",
    "검증셋 msle 0.0005045399261525063\n",
    "\n",
    "AdaBoostRegressor\n",
    "학습셋 설명력 0.9804426291558829\n",
    "검증셋 설명력 0.9819935005374005\n",
    "검증셋 msle 0.5078807945567321\n",
    "\n",
    "BaggingRegressor\n",
    "학습셋 설명력 0.9999035034524605\n",
    "검증셋 설명력 0.9993172164247892\n",
    "검증셋 msle 0.00020697400787588686\n",
    "\n",
    "ExtraTreesRegressor\n",
    "학습셋 설명력 0.9999999999749478\n",
    "검증셋 설명력 0.9996752315651899\n",
    "검증셋 msle 6.005228399762248e-05\n",
    "0.00006005228399762248\n",
    "GradientBoostingRegressor\n",
    "학습셋 설명력 0.9993160425971864\n",
    "검증셋 설명력 0.9990189983569386\n",
    "검증셋 msle 0.0037368855560598892\n",
    "\n",
    "RandomForestRegressor\n",
    "학습셋 설명력 0.9999552286983978\n",
    "검증셋 설명력 0.9993400789959608\n",
    "검증셋 msle 0.0001629502041205388\n",
    "\n",
    "ExtraTreesRegressor => 랜덤 포레스트보다 변수 중요도를 잘본다..?\n",
    "어쨴듯 해당 모델이 msle가 검증셋이 가장 작게 나왔기 때문에 해당 모델 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = len(X_train)\n",
    "x_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "x_all['datetime'] = pd.to_datetime(x_all['datetime'])\n",
    "\n",
    "\n",
    "x_all_1 = pd.get_dummies(x_all)\n",
    "del x_all_1['datetime']\n",
    "X_columns = x_all_1.columns\n",
    "\n",
    "x_all_1 = pd.DataFrame(x_all_1 , columns = X_columns)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "x_all_2 = Scaler.fit_transform(x_all_1)\n",
    "# x_all_2 = pd.DataFrame(x_all_2 , columns = X_columns)\n",
    "\n",
    "x_train =  x_all_2[:X_train_len]\n",
    "x_test = x_all_2[X_train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999352385395511\n",
      "0.9996688076004838\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "model.fit(x_train,y_train['count'])\n",
    "pred = model.predict(x_test)\n",
    "print(model.score(x_train,y_train['count']))\n",
    "\n",
    "print(model.score(x_test,y_test['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"index\":y_test.id,\"pred\":pred})\n",
    "df = df.set_index(df['index'],drop = True,inplace=False)\n",
    "del df[\"index\"]\n",
    "df.to_csv(\"수험번호.csv\",index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999352385395511\n",
      "0.9996688076004838\n",
      "1.4631759227449768\n",
      "3.246032707799647\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# model = XGBRegressor(learning_rate = 0.1,booster = \"gblinear\") # @@ gbliner를 이용하자??\n",
    "model = XGBRegressor()\n",
    "# model = XGBRegressor(booster = \"gblinear\")\n",
    "model.fit(x_train,y_train['count'])\n",
    "\n",
    "print(model.score(x_train,y_train['count']))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "# pred\n",
    "\n",
    "print(model.score(x_test,y_test['count']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train['count'], model.predict(x_train)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test['count'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐글 풀이\n",
    "# # print(X_train.head())\n",
    "# # print(y_train.head())\n",
    "\n",
    "# X_train_len = len(X_train)\n",
    "# X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "# # print(X_all.info())\n",
    "# # print(X_all[\"datetime\"])\n",
    "# # print(X_all.head())\n",
    "# # print(X_all.tail())\n",
    "# # print(X_all.columns)\n",
    "\n",
    "# # print(pd.to_datetime(X_all[\"datetime\"]))\n",
    "# Day_time = X_all[[\"datetime\"]]\n",
    "# X_all = X_all.drop([\"datetime\"],axis = 1)\n",
    "# X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "# # X_all_1 = pd.concat([X_all_1,Day_time],axis = 1)\n",
    "# print(X_all_1.head())\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# SS = StandardScaler()\n",
    "# X_all_2 = SS.fit_transform(X_all_1)\n",
    "\n",
    "\n",
    "\n",
    "# X_train_2 = X_all_2[:X_train_len]\n",
    "# X_test_2 = X_all_2[X_train_len:]\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "# model = XGBRegressor()\n",
    "\n",
    "# model.fit(X_train_2,y_train[\"count\"])\n",
    "\n",
    "# print(model.score(X_train_2,y_train[\"count\"]))\n",
    "\n",
    "# pred = model.predict(X_test_2)\n",
    "\n",
    "# print(model.score(X_test_2,y_test[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MOCK EXAM1] TYPE2. HR-DATA / 작업형2 모의고사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+ \"\\\\hr_data\\\\X_test.csv\")\n",
    "X_train = pd.read_csv(path+ \"\\\\hr_data\\\\X_train.csv\")\n",
    "y_train = pd.read_csv(path+ \"\\\\hr_data\\\\y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8521244388767095\n"
     ]
    }
   ],
   "source": [
    "tr_len = len(X_train)\n",
    "x_all = pd.concat([X_train , X_test],axis = 0)\n",
    "\n",
    "del x_all['enrollee_id']\n",
    "\n",
    "x_all_1 = pd.get_dummies(x_all)\n",
    "X_columns = x_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # @ \n",
    "Scaler = StandardScaler()\n",
    "Scaler.fit(x_all_1)\n",
    "x_all_2 = Scaler.transform(x_all_1)\n",
    "\n",
    "# x_all_2 = pd.DataFrame(x_all_2 ,columns = X_columns)\n",
    "x_train = x_all_2[:tr_len]\n",
    "x_test = x_all_2[tr_len:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8521244388767095\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train , y_train[\"target\"])\n",
    "print(model.score(x_train , y_train[\"target\"]))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "df = pd.DataFrame({\"predict\":pred})\n",
    "\n",
    "df['index'] = y_train['enrollee_id']\n",
    "df = df.set_index(df[\"index\"],drop = True,inplace = False)\n",
    "df.to_csv(\"수험번호.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9985906670842468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train , y_train[\"target\"])\n",
    "print(model.score(x_train , y_train[\"target\"]))\n",
    "\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaBoostClassifier',\n",
       " 'AdaBoostRegressor',\n",
       " 'BaggingClassifier',\n",
       " 'BaggingRegressor',\n",
       " 'BaseEnsemble',\n",
       " 'ExtraTreesClassifier',\n",
       " 'ExtraTreesRegressor',\n",
       " 'GradientBoostingClassifier',\n",
       " 'GradientBoostingRegressor',\n",
       " 'HistGradientBoostingClassifier',\n",
       " 'HistGradientBoostingRegressor',\n",
       " 'IsolationForest',\n",
       " 'RandomForestClassifier',\n",
       " 'RandomForestRegressor',\n",
       " 'RandomTreesEmbedding',\n",
       " 'StackingClassifier',\n",
       " 'StackingRegressor',\n",
       " 'VotingClassifier',\n",
       " 'VotingRegressor',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_bagging',\n",
       " '_base',\n",
       " '_forest',\n",
       " '_gb',\n",
       " '_gb_losses',\n",
       " '_gradient_boosting',\n",
       " '_hist_gradient_boosting',\n",
       " '_iforest',\n",
       " '_stacking',\n",
       " '_voting',\n",
       " '_weight_boosting']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시험환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 설명력(trian) 0.6774603174603174\n",
      "모델 설명력(test) 0.6657142857142857\n",
      "학습셋 roc 스코어 0.616267169404794\n",
      "검증셋 roc 스코어 0.6208727815670705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "tr_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_all = X_all.drop([\"cust_id\"],axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# print(X_all.describe())\n",
    "\n",
    "# print(X_all[\"환불금액\"].describe()) #환불금액에 0값대신 na값이 들어가있는것으로 추측할수 있음 0이 min이 아니니까.\n",
    "# print(dir(X_all))\n",
    "# 컬럼 \n",
    "# print(X_all.info())\n",
    "# print(help(X_all.select_dtypes))\n",
    "# na값 채움.\n",
    "col = X_all.select_dtypes(\"float64\").columns\n",
    "# print(col)\n",
    "for i in range(len(col)):\n",
    "\tX_all.loc[X_all[col[i]].isna()==True,col] = 0\n",
    "\t\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "# print(col.shape)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer\n",
    "Scaler = PowerTransformer()\n",
    "X_all_1 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_1, columns = col)\n",
    "\n",
    "# X_all_2 = X_all_1 # 랜포의 경우만\n",
    "\n",
    "\n",
    "x_train = X_all_2[:tr_len]\n",
    "x_test = X_all_2[tr_len:]\n",
    "\n",
    "#검증용\n",
    "a = int(np.floor(tr_len/10)) * 1\n",
    "a = tr_len - a\n",
    "x_train_1 = x_train[:a]\n",
    "x_val_1 = x_train[a:]\n",
    "y_train_1 = y_train[:a]\n",
    "y_val_1 = y_train[a:]\n",
    "\n",
    "# print(x_train_1)\n",
    "# print(x_val_1)\n",
    "# print(y_train_1)\n",
    "# print(y_val_1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier,RandomForestClassifier\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "# model = XGBRFClassifier(eval_metric = \"logloss\",use_label_encoder=False)\n",
    "# model = RandomForestClassifier(random_state=42, oob_score = False,n_estimators=50,max_features= 9)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_1,y_train_1[\"gender\"])\n",
    "print(\"모델 설명력(trian)\",model.score(x_train_1,y_train_1[\"gender\"]))\n",
    "pred = model.predict(x_val_1)\n",
    "print(\"모델 설명력(test)\",model.score(x_val_1,y_val_1[\"gender\"]))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"학습셋 roc 스코어\",roc_auc_score(y_train_1[\"gender\"],model.predict(x_train_1)))\n",
    "# print(pred)\n",
    "print(\"검증셋 roc 스코어\",roc_auc_score(y_val_1[\"gender\"],pred))\n",
    "\n",
    "# # 제출용 모델\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression()\n",
    "# model.fit(x_train,y_train[\"gender\"])\n",
    "# print(\"모델 설명력\",model.score(x_train,y_train[\"gender\"]))\n",
    "# pred = model.predict(x_test)\n",
    "# prob = model.predict_proba(x_test)[:,1]\n",
    "# df = pd.DataFrame({\"cust_id\":X_test.cust_id,\"gender\":prob}).reset_index(drop= True).to_csv(\"수험번호.csv\",index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "모델 설명력(trian) 0.9993650793650793\n",
    "모델 설명력(test) 0.6428571428571429\n",
    "학습셋 roc 스코어 0.9994936708860759\n",
    "검증셋 roc 스코어 0.5982727612066918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "모델 설명력(trian) 0.9993650793650793\n",
    "모델 설명력(test) 0.6428571428571429\n",
    "학습셋 roc 스코어 0.9994936708860759\n",
    "검증셋 roc 스코어 0.5982727612066918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestClassifier in module sklearn.ensemble._forest object:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      " |      tree classifiers.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |      \n",
      " |      Number of features when fitting the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-76-c59a0a5138b7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-76-c59a0a5138b7>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    모델 설명력(trian) 0.6923809523809524\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "XGBRFClassifier\n",
    "모델 설명력(trian) 0.6923809523809524\n",
    "모델 설명력(test) 0.6028571428571429\n",
    "학습셋 roc 스코어 0.6393697818475625\n",
    "검증셋 roc 스코어 0.5567036546879772\n",
    "\n",
    "LogisticRegression\n",
    "모델 설명력(trian) 0.6752380952380952\n",
    "모델 설명력(test) 0.6628571428571428\n",
    "학습셋 roc 스코어 0.6019122003770536\n",
    "검증셋 roc 스코어 0.6127116631036005\n",
    "\n",
    "AdaBoostClassifier\n",
    "모델 설명력(trian) 0.686984126984127\n",
    "모델 설명력(test) 0.6457142857142857\n",
    "학습셋 roc 스코어 0.6348936170212766\n",
    "검증셋 roc 스코어 0.6029726152906443\n",
    "\n",
    "BaggingClassifier\n",
    "모델 설명력(trian) 0.9771428571428571\n",
    "모델 설명력(test) 0.6142857142857143\n",
    "학습셋 roc 스코어 0.9712577430649071\n",
    "검증셋 roc 스코어 0.5685805422647527\n",
    "\n",
    "ExtraTreesClassifier\n",
    "모델 설명력(trian) 0.9993650793650793\n",
    "모델 설명력(test) 0.62\n",
    "학습셋 roc 스코어 0.9991489361702128\n",
    "검증셋 roc 스코어 0.577980250432658\n",
    "\n",
    "GradientBoostingClassifier\n",
    "모델 설명력(trian) 0.7158730158730159\n",
    "모델 설명력(test) 0.6485714285714286\n",
    "학습셋 roc 스코어 0.6586210611365473\n",
    "검증셋 roc 스코어 0.6007499406155622\n",
    "\n",
    "RandomForestClassifier\n",
    "모델 설명력(trian) 0.9993650793650793\n",
    "모델 설명력(test) 0.6314285714285715\n",
    "학습셋 roc 스코어 0.9994936708860759\n",
    "검증셋 roc 스코어 0.5898571380094336\n",
    "\n",
    "XGBRFClassifier\n",
    "모델 설명력(trian) 0.6923809523809524\n",
    "모델 설명력(test) 0.6028571428571429\n",
    "학습셋 roc 스코어 0.6393697818475625\n",
    "검증셋 roc 스코어 0.5567036546879772\n",
    "\n",
    "XGBClassifier\n",
    "모델 설명력(trian) 0.9387301587301587\n",
    "모델 설명력(test) 0.6228571428571429\n",
    "학습셋 roc 스코어 0.9323511984917856\n",
    "검증셋 roc 스코어 0.5838338593097832\n",
    "\n",
    "논리대로라면 LogisticRegression 을 선택하는것이 맞으나.. 왤케 .. 맘에 안들지...ㅎ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기']\n",
      "0.932\n"
     ]
    }
   ],
   "source": [
    "## xgboost 검증셋 없이 학습\n",
    "\n",
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "# 사용자 코딩\n",
    "\n",
    "# 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "import numpy as np\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_train_len = len(X_train)\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "# print(len(X_all))\n",
    "\n",
    "train_all = pd.concat([X_train,y_train],axis = 1)\n",
    "# print(len(train_all))\n",
    "# print(X_all)\n",
    "\n",
    "\n",
    "#상관분석 후 사용할 컬럼 지정\n",
    "aa = pd.DataFrame(abs(train_all.corr()['gender'])>0.02)\n",
    "use_col = list(aa.loc[aa['gender']==True][:-1].index)\n",
    "print(use_col)\n",
    "######\n",
    "\n",
    "#더미화 및 데이터 확인\n",
    "X_all_1 = X_all[use_col]\n",
    "# print(X_all_1.info())\n",
    "X_all_2 = pd.get_dummies(X_all_1)\n",
    "# print(X_all_2.info())\n",
    "\n",
    "#정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_2)\n",
    "# print(X_all_2)\n",
    "\n",
    "\n",
    "#데이터 프레임 다시 나누기\n",
    "# print(X_train_len)\n",
    "\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "y_train_2 = y_train['gender']\n",
    "# print(y_train_2)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# print(dir(XGBClassifier))\n",
    "# model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "# print(help(model))\n",
    "model.fit(X_train_2,y_train_2)\n",
    "\n",
    "print(model.score(X_train_2,y_train['gender'])) # 모델 점수\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "# prob = model.predict_proba(X_test_2)[:,:1]\n",
    "pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기']\n",
      "500\n",
      "[0]\tvalidation_0-logloss:0.67263\n",
      "[1]\tvalidation_0-logloss:0.66471\n",
      "[2]\tvalidation_0-logloss:0.66166\n",
      "[3]\tvalidation_0-logloss:0.65971\n",
      "[4]\tvalidation_0-logloss:0.65713\n",
      "[5]\tvalidation_0-logloss:0.65694\n",
      "[6]\tvalidation_0-logloss:0.65899\n",
      "[7]\tvalidation_0-logloss:0.65547\n",
      "[8]\tvalidation_0-logloss:0.65613\n",
      "[9]\tvalidation_0-logloss:0.66075\n",
      "[10]\tvalidation_0-logloss:0.66352\n",
      "[11]\tvalidation_0-logloss:0.66451\n",
      "[12]\tvalidation_0-logloss:0.66231\n",
      "[13]\tvalidation_0-logloss:0.66534\n",
      "[14]\tvalidation_0-logloss:0.66675\n",
      "[15]\tvalidation_0-logloss:0.66750\n",
      "[16]\tvalidation_0-logloss:0.66787\n",
      "[17]\tvalidation_0-logloss:0.66857\n",
      "[18]\tvalidation_0-logloss:0.66907\n",
      "[19]\tvalidation_0-logloss:0.67068\n",
      "[20]\tvalidation_0-logloss:0.67277\n",
      "[21]\tvalidation_0-logloss:0.67333\n",
      "[22]\tvalidation_0-logloss:0.67401\n",
      "[23]\tvalidation_0-logloss:0.67356\n",
      "[24]\tvalidation_0-logloss:0.67435\n",
      "[25]\tvalidation_0-logloss:0.67556\n",
      "[26]\tvalidation_0-logloss:0.67727\n",
      "[27]\tvalidation_0-logloss:0.67919\n",
      "[28]\tvalidation_0-logloss:0.67985\n",
      "[29]\tvalidation_0-logloss:0.67821\n",
      "[30]\tvalidation_0-logloss:0.67879\n",
      "[31]\tvalidation_0-logloss:0.67929\n",
      "[32]\tvalidation_0-logloss:0.68154\n",
      "[33]\tvalidation_0-logloss:0.68416\n",
      "[34]\tvalidation_0-logloss:0.68510\n",
      "[35]\tvalidation_0-logloss:0.68435\n",
      "[36]\tvalidation_0-logloss:0.68383\n",
      "[37]\tvalidation_0-logloss:0.68472\n",
      "[38]\tvalidation_0-logloss:0.68548\n",
      "[39]\tvalidation_0-logloss:0.68400\n",
      "[40]\tvalidation_0-logloss:0.68376\n",
      "[41]\tvalidation_0-logloss:0.68501\n",
      "[42]\tvalidation_0-logloss:0.68513\n",
      "[43]\tvalidation_0-logloss:0.68587\n",
      "[44]\tvalidation_0-logloss:0.68688\n",
      "[45]\tvalidation_0-logloss:0.68703\n",
      "[46]\tvalidation_0-logloss:0.68733\n",
      "[47]\tvalidation_0-logloss:0.68759\n",
      "[48]\tvalidation_0-logloss:0.68833\n",
      "[49]\tvalidation_0-logloss:0.68804\n",
      "[50]\tvalidation_0-logloss:0.69443\n",
      "[51]\tvalidation_0-logloss:0.69540\n",
      "[52]\tvalidation_0-logloss:0.69973\n",
      "[53]\tvalidation_0-logloss:0.70026\n",
      "[54]\tvalidation_0-logloss:0.70025\n",
      "[55]\tvalidation_0-logloss:0.70445\n",
      "[56]\tvalidation_0-logloss:0.70515\n",
      "[57]\tvalidation_0-logloss:0.70639\n",
      "[58]\tvalidation_0-logloss:0.70671\n",
      "[59]\tvalidation_0-logloss:0.70804\n",
      "[60]\tvalidation_0-logloss:0.70905\n",
      "[61]\tvalidation_0-logloss:0.71108\n",
      "[62]\tvalidation_0-logloss:0.71275\n",
      "[63]\tvalidation_0-logloss:0.71339\n",
      "[64]\tvalidation_0-logloss:0.71389\n",
      "[65]\tvalidation_0-logloss:0.71556\n",
      "[66]\tvalidation_0-logloss:0.71705\n",
      "[67]\tvalidation_0-logloss:0.71690\n",
      "[68]\tvalidation_0-logloss:0.71889\n",
      "[69]\tvalidation_0-logloss:0.72016\n",
      "[70]\tvalidation_0-logloss:0.72524\n",
      "[71]\tvalidation_0-logloss:0.72626\n",
      "[72]\tvalidation_0-logloss:0.72749\n",
      "[73]\tvalidation_0-logloss:0.72738\n",
      "[74]\tvalidation_0-logloss:0.72907\n",
      "[75]\tvalidation_0-logloss:0.72815\n",
      "[76]\tvalidation_0-logloss:0.72927\n",
      "[77]\tvalidation_0-logloss:0.72959\n",
      "[78]\tvalidation_0-logloss:0.73216\n",
      "[79]\tvalidation_0-logloss:0.73368\n",
      "[80]\tvalidation_0-logloss:0.73567\n",
      "[81]\tvalidation_0-logloss:0.73607\n",
      "[82]\tvalidation_0-logloss:0.73542\n",
      "[83]\tvalidation_0-logloss:0.73562\n",
      "[84]\tvalidation_0-logloss:0.73722\n",
      "[85]\tvalidation_0-logloss:0.73665\n",
      "[86]\tvalidation_0-logloss:0.73886\n",
      "[87]\tvalidation_0-logloss:0.74023\n",
      "[88]\tvalidation_0-logloss:0.73986\n",
      "[89]\tvalidation_0-logloss:0.74101\n",
      "[90]\tvalidation_0-logloss:0.74286\n",
      "[91]\tvalidation_0-logloss:0.74326\n",
      "[92]\tvalidation_0-logloss:0.74367\n",
      "[93]\tvalidation_0-logloss:0.74496\n",
      "[94]\tvalidation_0-logloss:0.74469\n",
      "[95]\tvalidation_0-logloss:0.74796\n",
      "[96]\tvalidation_0-logloss:0.74825\n",
      "[97]\tvalidation_0-logloss:0.74855\n",
      "[98]\tvalidation_0-logloss:0.74899\n",
      "[99]\tvalidation_0-logloss:0.75178\n",
      "0.943\n"
     ]
    }
   ],
   "source": [
    "######################################################################### \n",
    "## xgboost 검증셋 추가\n",
    "\n",
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "# 사용자 코딩\n",
    "\n",
    "# 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "import numpy as np\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_train_len = len(X_train)\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "# print(len(X_all))\n",
    "\n",
    "train_all = pd.concat([X_train,y_train],axis = 1)\n",
    "# print(len(train_all))\n",
    "# print(X_all)\n",
    "\n",
    "\n",
    "#상관분석 후 사용할 컬럼 지정\n",
    "aa = pd.DataFrame(abs(train_all.corr()['gender'])>0.02)\n",
    "use_col = list(aa.loc[aa['gender']==True][:-1].index)\n",
    "print(use_col)\n",
    "######\n",
    "\n",
    "#더미화 및 데이터 확인\n",
    "X_all_1 = X_all[use_col]\n",
    "# print(X_all_1.info())\n",
    "X_all_2 = pd.get_dummies(X_all_1)\n",
    "# print(X_all_2.info())\n",
    "\n",
    "#정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_2)\n",
    "# print(X_all_2)\n",
    "\n",
    "\n",
    "#데이터 프레임 다시 나누기\n",
    "# print(X_train_len)\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "y_train_2 = y_train['gender']\n",
    "\n",
    "x_validation = X_train_2[-500:]\n",
    "y_validation = y_train_2[-500:]\n",
    "print(len(x_validation))\n",
    "\n",
    "X_train_2 = X_train_2[:-500]\n",
    "y_train_2 = y_train_2[:-500]\n",
    "\n",
    "# print(y_train_2)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# print(dir(XGBClassifier))\n",
    "# model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "# print(help(model))\n",
    "# model.fit(X_train_2,y_train_2)\n",
    "model.fit(X_train_2,y_train_2,eval_set=[(x_validation, y_validation)])\n",
    "\n",
    "print(model.score(X_train_2,y_train_2)) # 모델 점수\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "# prob = model.predict_proba(X_test_2)[:,:1]\n",
    "pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.67263\n",
      "[1]\tvalidation_0-logloss:0.66471\n",
      "[2]\tvalidation_0-logloss:0.66166\n",
      "[3]\tvalidation_0-logloss:0.65971\n",
      "[4]\tvalidation_0-logloss:0.65713\n",
      "[5]\tvalidation_0-logloss:0.65694\n",
      "[6]\tvalidation_0-logloss:0.65899\n",
      "[7]\tvalidation_0-logloss:0.65547\n",
      "[8]\tvalidation_0-logloss:0.65613\n",
      "[9]\tvalidation_0-logloss:0.66075\n",
      "[10]\tvalidation_0-logloss:0.66352\n",
      "[11]\tvalidation_0-logloss:0.66451\n",
      "[12]\tvalidation_0-logloss:0.66231\n",
      "[13]\tvalidation_0-logloss:0.66534\n",
      "[14]\tvalidation_0-logloss:0.66675\n",
      "[15]\tvalidation_0-logloss:0.66750\n",
      "[16]\tvalidation_0-logloss:0.66787\n",
      "[17]\tvalidation_0-logloss:0.66857\n",
      "[18]\tvalidation_0-logloss:0.66907\n",
      "[19]\tvalidation_0-logloss:0.67068\n",
      "[20]\tvalidation_0-logloss:0.67277\n",
      "[21]\tvalidation_0-logloss:0.67333\n",
      "[22]\tvalidation_0-logloss:0.67401\n",
      "[23]\tvalidation_0-logloss:0.67356\n",
      "[24]\tvalidation_0-logloss:0.67435\n",
      "[25]\tvalidation_0-logloss:0.67556\n",
      "[26]\tvalidation_0-logloss:0.67727\n",
      "[27]\tvalidation_0-logloss:0.67919\n",
      "[28]\tvalidation_0-logloss:0.67985\n",
      "[29]\tvalidation_0-logloss:0.67821\n",
      "[30]\tvalidation_0-logloss:0.67879\n",
      "[31]\tvalidation_0-logloss:0.67929\n",
      "[32]\tvalidation_0-logloss:0.68154\n",
      "[33]\tvalidation_0-logloss:0.68416\n",
      "[34]\tvalidation_0-logloss:0.68510\n",
      "[35]\tvalidation_0-logloss:0.68435\n",
      "[36]\tvalidation_0-logloss:0.68383\n",
      "[37]\tvalidation_0-logloss:0.68472\n",
      "[38]\tvalidation_0-logloss:0.68548\n",
      "[39]\tvalidation_0-logloss:0.68400\n",
      "[40]\tvalidation_0-logloss:0.68376\n",
      "[41]\tvalidation_0-logloss:0.68501\n",
      "[42]\tvalidation_0-logloss:0.68513\n",
      "[43]\tvalidation_0-logloss:0.68587\n",
      "[44]\tvalidation_0-logloss:0.68688\n",
      "[45]\tvalidation_0-logloss:0.68703\n",
      "[46]\tvalidation_0-logloss:0.68733\n",
      "[47]\tvalidation_0-logloss:0.68759\n",
      "[48]\tvalidation_0-logloss:0.68833\n",
      "[49]\tvalidation_0-logloss:0.68804\n",
      "[50]\tvalidation_0-logloss:0.69443\n",
      "[51]\tvalidation_0-logloss:0.69540\n",
      "[52]\tvalidation_0-logloss:0.69973\n",
      "[53]\tvalidation_0-logloss:0.70026\n",
      "[54]\tvalidation_0-logloss:0.70025\n",
      "[55]\tvalidation_0-logloss:0.70445\n",
      "[56]\tvalidation_0-logloss:0.70515\n",
      "[57]\tvalidation_0-logloss:0.70639\n",
      "[58]\tvalidation_0-logloss:0.70671\n",
      "[59]\tvalidation_0-logloss:0.70804\n",
      "[60]\tvalidation_0-logloss:0.70905\n",
      "[61]\tvalidation_0-logloss:0.71108\n",
      "[62]\tvalidation_0-logloss:0.71275\n",
      "[63]\tvalidation_0-logloss:0.71339\n",
      "[64]\tvalidation_0-logloss:0.71389\n",
      "[65]\tvalidation_0-logloss:0.71556\n",
      "[66]\tvalidation_0-logloss:0.71705\n",
      "[67]\tvalidation_0-logloss:0.71690\n",
      "[68]\tvalidation_0-logloss:0.71889\n",
      "[69]\tvalidation_0-logloss:0.72016\n",
      "[70]\tvalidation_0-logloss:0.72524\n",
      "[71]\tvalidation_0-logloss:0.72626\n",
      "[72]\tvalidation_0-logloss:0.72749\n",
      "[73]\tvalidation_0-logloss:0.72738\n",
      "[74]\tvalidation_0-logloss:0.72907\n",
      "[75]\tvalidation_0-logloss:0.72815\n",
      "[76]\tvalidation_0-logloss:0.72927\n",
      "[77]\tvalidation_0-logloss:0.72959\n",
      "[78]\tvalidation_0-logloss:0.73216\n",
      "[79]\tvalidation_0-logloss:0.73368\n",
      "[80]\tvalidation_0-logloss:0.73567\n",
      "[81]\tvalidation_0-logloss:0.73607\n",
      "[82]\tvalidation_0-logloss:0.73542\n",
      "[83]\tvalidation_0-logloss:0.73562\n",
      "[84]\tvalidation_0-logloss:0.73722\n",
      "[85]\tvalidation_0-logloss:0.73665\n",
      "[86]\tvalidation_0-logloss:0.73886\n",
      "[87]\tvalidation_0-logloss:0.74023\n",
      "[88]\tvalidation_0-logloss:0.73986\n",
      "[89]\tvalidation_0-logloss:0.74101\n",
      "[90]\tvalidation_0-logloss:0.74286\n",
      "[91]\tvalidation_0-logloss:0.74326\n",
      "[92]\tvalidation_0-logloss:0.74367\n",
      "[93]\tvalidation_0-logloss:0.74496\n",
      "[94]\tvalidation_0-logloss:0.74469\n",
      "[95]\tvalidation_0-logloss:0.74796\n",
      "[96]\tvalidation_0-logloss:0.74825\n",
      "[97]\tvalidation_0-logloss:0.74855\n",
      "[98]\tvalidation_0-logloss:0.74899\n",
      "[99]\tvalidation_0-logloss:0.75178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='logloss', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# print(dir(XGBClassifier))\n",
    "# model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "# print(help(model))\n",
    "# model.fit(X_train_2,y_train_2)\n",
    "model.fit(X_train_2,y_train_2,eval_set=[(x_validation, y_validation)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5644519875538694 0.4350282485875706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred= model.predict(x_validation)\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.roc_auc_score(y_validation,pred),sklearn.metrics.f1_score(y_validation,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중분류\n",
    "평가지표 : f1_score _ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris# scikit-learn에 있는 Iris 데이터셋을 가져오기 위해서 load_iris를 import한다. \n",
    "iris = load_iris()# 말그대로 iris 데이터셋을 로드한다.\n",
    "print(iris['DESCR'])\n",
    "\n",
    "data = iris['data']\n",
    "feature_names = iris['feature_names']\n",
    "target = iris['target']\n",
    "df_iris = pd.DataFrame(data, columns = feature_names)\n",
    "df_iris['target'] = target\n",
    "df_iris.loc[df_iris[\"target\"]==0,\"target\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_iris.drop('target', 1),df_iris['target'],random_state=2021)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "y_train = pd.DataFrame({\"target\":y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.loc[y_train[\"target\"]==1,target] = \"1\"\n",
    "y_train.loc[y_train[\"target\"]==2,target] = \"2\"\n",
    "y_train.loc[y_train[\"target\"]==3,target] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_train[\"target\"] = y_train[\"target\"].astype(\"object\")\n",
    "tr_len = len(x_train)\n",
    "\n",
    "X_all = pd.concat([x_train,x_test],axis = 0)\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "col = X_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer,LabelEncoder,StandardScaler\n",
    "Scaler = MinMaxScaler()\n",
    "X_all_1 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_1, columns = col)\n",
    "\n",
    "x_train = X_all_2[:tr_len]\n",
    "x_test = X_all_2[tr_len:]\n",
    "\n",
    "# print(y_train.value_counts())\n",
    "# print(y_train.info())\n",
    "LE = LabelEncoder()\n",
    "y_train[\"target\"]=LE.fit_transform(y_train[\"target\"])\n",
    "# print(y_train.value_counts())\n",
    "# print(y_train.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 점수 (학습) 0.9702970297029703\n",
      "모델 점수 (검증) 1.0\n",
      "micro f1 스코어 1.0\n",
      "macro f1 스코어 1.0\n",
      "     실제  예측\n",
      "102   1   1\n",
      "93    0   0\n",
      "21    2   2\n",
      "62    0   0\n",
      "44    2   2\n",
      "94    0   0\n",
      "109   1   1\n",
      "128   1   1\n",
      "57    0   0\n",
      "85    0   0\n",
      "116   1   1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = int(np.floor(tr_len/10)) * 1\n",
    "a = tr_len - a\n",
    "\n",
    "x_train_1 = x_train[:a]\n",
    "x_val_1 = x_train[a:]\n",
    "\n",
    "y_train_1 = y_train[:a]\n",
    "y_val_1 = y_train[a:]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier,RandomForestClassifier\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(x_train_1,y_train_1[\"target\"])\n",
    "pred= model.predict(x_val_1)\n",
    "# pred = LE.inverse_transform(pred) # 라벨인코더 역변환.\n",
    "print(\"모델 점수 (학습)\",model.score(x_train_1,y_train_1[\"target\"]))\n",
    "print(\"모델 점수 (검증)\",model.score(x_val_1,y_val_1[\"target\"]))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"micro f1 스코어\",f1_score(y_val_1[\"target\"],pred ,average = \"micro\"))\n",
    "print(\"macro f1 스코어\",f1_score(y_val_1[\"target\"],pred ,average = \"macro\"))\n",
    "\n",
    "df1 = pd.DataFrame({\"실제\":y_val_1[\"target\"],\"예측\":pred})\n",
    "print(df1)\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# print(\"그냥해보는 roc 스코어\",roc_auc_score(y_val_1[\"target\"],pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRFClassifier\n",
    "모델 점수 (학습) 0.9746835443037974\n",
    "모델 점수 (검증) 0.9393939393939394\n",
    "micro f1 스코어 0.9393939393939394\n",
    "macro f1 스코어 0.942857142857143\n",
    "\n",
    "AdaBoostClassifier\n",
    "모델 점수 (학습) 0.9746835443037974\n",
    "모델 점수 (검증) 0.9393939393939394\n",
    "micro f1 스코어 0.9393939393939394\n",
    "macro f1 스코어 0.942857142857143\n",
    "\n",
    "BaggingClassifier\n",
    "\n",
    "모델 점수 (학습) 1.0\n",
    "모델 점수 (검증) 0.9393939393939394\n",
    "micro f1 스코어 0.9393939393939394\n",
    "macro f1 스코어 0.942857142857143\n",
    "\n",
    "\n",
    "ExtraTreesClassifier\n",
    "모델 점수 (학습) 1.0\n",
    "모델 점수 (검증) 0.9393939393939394\n",
    "micro f1 스코어 0.9393939393939394\n",
    "macro f1 스코어 0.942857142857143\n",
    "\n",
    "GradientBoostingClassifier\n",
    "모델 점수 (학습) 1.0\n",
    "모델 점수 (검증) 0.9393939393939394\n",
    "micro f1 스코어 0.9393939393939394\n",
    "macro f1 스코어 0.942857142857143\n",
    "\n",
    "XGBClassifier\n",
    "모델 점수 (학습) 1.0\n",
    "모델 점수 (검증) 0.9090909090909091\n",
    "micro f1 스코어 0.9090909090909091\n",
    "macro f1 스코어 0.9128856624319419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:42:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "모델 설명력 0.9642857142857143\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38 entries, 0 to 37\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   pred    38 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 432.0+ bytes\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38 entries, 0 to 37\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   pred    38 non-null     int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 432.0 bytes\n",
      "None\n",
      "pred\n",
      "1       16\n",
      "3       16\n",
      "2        6\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 제출용 모델\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "model = XGBRFClassifier()\n",
    "model.fit(x_train,y_train[\"target\"])\n",
    "pred= model.predict(x_test)\n",
    "print(\"모델 설명력\",model.score(x_train,y_train[\"target\"]))\n",
    "\n",
    "pred = LE.inverse_transform(pred) # 라벨인코더 역변환.\n",
    "df = pd.DataFrame({\"pred\":pred})\n",
    "df[\"pred\"] = df[\"pred\"].astype(\"object\")\n",
    "df.to_csv(\"수험번호.csv\",index =False)\n",
    "print(df.info())\n",
    "df = pd.read_csv(\"수험번호.csv\")\n",
    "print(df.info())\n",
    "print(df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
