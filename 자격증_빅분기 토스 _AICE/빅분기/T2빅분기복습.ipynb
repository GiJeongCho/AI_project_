{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\Happy\\\\Desktop\\\\빅분기\\\\input\"\n",
    "# .iloc로 하기 df[0] => 인덱스로 확인함.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-1\n",
    "생존여부 예측모델 만들기\n",
    "학습용 데이터 (X_train, y_train)을 이용하여 생존 예측 모형을 만든 후, 이를 평가용 데이터(X_test)에 적용하여 얻은 예측값을 다음과 같은 형식의 CSV파일로 생성하시오(제출한 모델의 성능은 accuracy 평가지표에 따라 채점)\n",
    "(가) 제공 데이터 목록\n",
    "\n",
    "y_train: 생존여부(학습용)\n",
    "X_trian, X_test : 승객 정보 (학습용 및 평가용)\n",
    "(나) 데이터 형식 및 내용\n",
    "\n",
    "y_trian (712명 데이터)\n",
    "시험환경 세팅은 예시문제와 동일한 형태의 X_train, y_train, X_test 데이터를 만들기 위함임\n",
    "\n",
    "유의사항\n",
    "성능이 우수한 예측모형을 구축하기 위해서는 적절한 데이터 전처리, 피처엔지니어링, 분류알고리즘, 하이퍼파라미터 튜닝, 모형 앙상블 등이 수반되어야 한다.\n",
    "수험번호.csv파일이 만들어지도록 코드를 제출한다.\n",
    "제출한 모델의 성능은 accuracy로 평가함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 11), (179, 11), (712, 2), (179, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\titanic\\\\train.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='Survived', id_name='PassengerId')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x_len = len(X_train)\n",
    "X_all = pd.concat([X_train,X_test] , axis=0)\n",
    "# train_all = pd.concat([X_train,y_train] , axis=1)\n",
    "\n",
    "# X_train = X_all[train_list][:x_len]\n",
    "# X_test = X_all[train_list][x_len:]\n",
    "\n",
    "X_all = pd.get_dummies(X_all)\n",
    "\n",
    "X_train = X_all[:x_len]\n",
    "X_test = X_all[x_len:]\n",
    "\n",
    "train_all = pd.concat([X_train,y_train] , axis=1)\n",
    "\n",
    "aa = pd.DataFrame(abs(train_all.corr()['Survived']))\n",
    "train_list = list(aa.loc[(aa['Survived']>0.2)][:-1].index)\n",
    "\n",
    "X_train = X_train[train_list]\n",
    "X_test = X_test[train_list]\n",
    "\n",
    "X_all = pd.concat([X_train,X_test] , axis=0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all = SS.fit_transform(X_all)\n",
    "\n",
    "X_train = X_all[:x_len]\n",
    "X_test = X_all[x_len:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901685393258427\n",
      "0.776536312849162\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "# model = XGBClassifier(learning_rate=0.01,objective= 'binary:logistic',eval_metric = \"merror\", booster='gbtree',use_label_encoder= True,)\n",
    "model.fit(X_train,y_train['Survived'],verbose = 1) #@ fit에 ,verbose = 1 로 두면 \n",
    "print(model.score(X_train,y_train['Survived']))\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "#@@ 저장 부분 잘 기억하기\n",
    "pd.DataFrame({'cust_id': y_test.PassengerId, 'Survived': pred}).to_csv('수험번호.csv', index=False)\n",
    "# pd.read_csv('수험번호.csv')\n",
    "\n",
    "print(model.score(X_test,y_test['Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6193820224719101\n",
      "0.6033519553072626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Happy\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "# from lightgbm import LGBMClassifier\n",
    "# model = LGBMClassifier(learning_rate = 0.001)\n",
    "# # model = XGBClassifier(learning_rate=0.01,objective= 'binary:logistic',eval_metric = \"merror\", booster='gbtree',use_label_encoder= True,)\n",
    "# model.fit(X_train,y_train['Survived'],verbose = 1) #@ fit에 ,verbose = 1 로 두면 \n",
    "# print(model.score(X_train,y_train['Survived']))\n",
    "\n",
    "# pred = model.predict(X_test)\n",
    "# #@@ 저장 부분 잘 기억하기\n",
    "# pd.DataFrame({'cust_id': y_test.PassengerId, 'Survived': pred}).to_csv('수험번호.csv', index=False)\n",
    "# # pd.read_csv('수험번호.csv')\n",
    "\n",
    "# print(model.score(X_test,y_test['Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.776536312849162\n",
      "0.7183098591549296 0.7183098591549296 0.06624934793948867 [[[51 20]\n",
      "  [20 88]]\n",
      "\n",
      " [[88 20]\n",
      "  [20 51]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7665623369848722"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.score(X_test,y_test['Survived']))\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_test['Survived'],pred), \\\n",
    "sklearn.metrics.recall_score(y_test['Survived'],pred), \\\n",
    "sklearn.metrics.r2_score(y_test['Survived'],pred), \\\n",
    "sklearn.metrics.multilabel_confusion_matrix(y_test['Survived'],pred))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sklearn.metrics.roc_auc_score(y_test['Survived'],pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9199438202247191\n",
      "0.7541899441340782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train,y_train['Survived'])\n",
    "\n",
    "print(model.score(X_train,y_train['Survived']))\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(model.score(X_test,y_test['Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-16a5598eed0d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-16a5598eed0d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    1.더미화\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1.더미화\n",
    "2.상관분석 후 필요한 컬럼만 뽑음\n",
    "3.필요한 컬럼들 정규화.\n",
    "4.예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 예전 내 정답\n",
    "import pandas as pd\n",
    "df = pd.read_csv(path+ \"\\\\titanic\\\\train.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n",
    "y_train = X_train[['PassengerId', 'Survived']]\n",
    "X_train = X_train.drop(columns=['PassengerId', 'Survived'])\n",
    "y_test = X_test[['PassengerId', 'Survived']]\n",
    "X_test = X_test.drop(columns=['PassengerId', 'Survived'])\n",
    "\n",
    "\n",
    "X_train_len = len(X_train)\n",
    "# print(y_train)\n",
    "\n",
    "X_all = pd.concat([X_train,X_test],axis= 0)\n",
    "# print(y_test.head())\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "###시간남으면 하기####\n",
    "\n",
    "X_train_1 = X_all_1[:X_train_len]\n",
    "X_test_1 = X_all_1[X_train_len:]\n",
    "train_all = pd.concat([X_train_1,y_train['Survived']],axis = 1)\n",
    "train_all_1 = pd.DataFrame(train_all.corr()['Survived'])\n",
    "# print(train_all_1[abs(train_all_1['Survived'])>0.01][:-1].index)\n",
    "Useing_col = list(train_all_1[abs(train_all_1['Survived'])>0.02][:-1].index)\n",
    "# print(len(Useing_col))\n",
    "X_train_1 = X_train_1[Useing_col]\n",
    "X_test_1 = X_test_1[Useing_col]\n",
    "X_all_1 = pd.concat([X_train_1,X_test_1],axis = 0)\n",
    "# print(X_all_1.info())\n",
    "# ###시간남으면 하기####\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_1)\n",
    "\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "# print(X_all_1.shape)\n",
    "# print(y_train['Survived'].shape)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(eval_metric =  \"logloss\",use_label_encoder = False)\n",
    "model.fit(X_train_2,y_train['Survived'])\n",
    "print(model.score(X_train_2,y_train['Survived']))\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "\n",
    "pd.DataFrame({'PassengerId':y_test.PassengerId,'pred or prob':pred}).to_csv(\"수험번호.csv\",index = False)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(model.predict(X_train_2),y_train['Survived']))\n",
    "# print(dir(sklearn.metrics))\n",
    "\n",
    "##여기는 수험자가 확인 불가능 영역\n",
    "print(model.score(X_test_2,y_test['Survived']))\n",
    "print(roc_auc_score(pred,y_test['Survived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-2. Pima Indians Diabetes(피마 인디언 당뇨병)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((614, 9), (154, 9), (614, 2), (154, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\archive\\\\diabetes.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='Outcome')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0.232571\n",
      "Glucose                     0.458378\n",
      "BloodPressure               0.048623\n",
      "SkinThickness               0.075839\n",
      "Insulin                     0.128257\n",
      "BMI                         0.296715\n",
      "DiabetesPedigreeFunction    0.174323\n",
      "Age                         0.218914\n",
      "Outcome                     1.000000\n",
      "Name: Outcome, dtype: float64\n",
      "0.9039087947882736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "tr_len = len(X_train) \n",
    "\n",
    "X_all = pd.concat([X_train, X_test],axis = 0)\n",
    "tr_col = X_all.columns\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sclaer = MinMaxScaler()\n",
    "X_all_2 = sclaer.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2, columns =tr_col )\n",
    "\n",
    "del X_all_2['id'] #id는 의미가 있을리가 없다\n",
    "\n",
    "X_train_2 = X_all_2[:tr_len]\n",
    "X_test_2 = X_all_2[tr_len:]\n",
    "\n",
    "y_train.reset_index(inplace = True,drop=True) # @@ 인덱스 번호로 concat(합쳐짐)합쳐지기 때문에 리셋 인덱스를 해야함\n",
    "train_all = pd.concat([X_train_2,y_train[['Outcome']]],axis = 1)\n",
    "\n",
    "print(train_all.corr()['Outcome']) # 상관성이 다들 어느정도 있어 보이니 넘어가자.\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(learning_rate = 0.01)\n",
    "model.fit(X_train_2 ,y_train['Outcome'], eval_metric  = 'logloss' )\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train_2 ,y_train['Outcome'])\n",
    "\n",
    "print(model.score(X_train_2 ,y_train['Outcome']))\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "\n",
    "answer = pd.DataFrame(pred,columns=[\"predict\"])\n",
    "answer.to_csv(\"수험번호.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7532467532467533\n",
      "0.6346153846153846 0.5789473684210527 -0.05841924398625409 [[[33 24]\n",
      "  [14 83]]\n",
      "\n",
      " [[83 14]\n",
      "  [24 33]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7173087357569181"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.score(X_test_2,y_test['Outcome']))\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.f1_score(y_test['Outcome'],pred), \\\n",
    "sklearn.metrics.recall_score(y_test['Outcome'],pred), \\\n",
    "sklearn.metrics.r2_score(y_test['Outcome'],pred), \\\n",
    "sklearn.metrics.multilabel_confusion_matrix(y_test['Outcome'],pred))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sklearn.metrics.roc_auc_score(y_test['Outcome'],pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-3. Adult Census Income Tutorial\n",
    "age: 나이\n",
    "workclass: 고용 형태\n",
    "fnlwgt: 사람의 대표성을 나타내는 가중치(final weight)\n",
    "education: 교육 수준\n",
    "education.num: 교육 수준 수치\n",
    "marital.status: 결혼 상태\n",
    "occupation: 업종\n",
    "relationship: 가족 관계\n",
    "race: 인종\n",
    "sex: 성별\n",
    "capital.gain: 양도 소득\n",
    "capital.loss: 양도 손실\n",
    "hours.per.week: 주당 근무 시간\n",
    "native.country: 국적\n",
    "income: 수익 (예측해야 하는 값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26048, 15), (6513, 15), (26048, 2), (6513, 2))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\archive\\\\adult.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='income', null_name='?')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9058277027027027\n",
      "0.871641332719177\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "['>50K' '<=50K' '>50K' ... '<=50K' '>50K' '>50K']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_tr_len = len(X_train) # 인덱스\n",
    "X_all = pd.concat([X_train,X_test])\n",
    "del X_all['id'] # 무조건 필요없는 id컬럼 드롭\n",
    "\n",
    "\n",
    "X_all = X_all.reset_index(drop=True)\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "X_tr_cal = X_all_1.columns # 컬럼\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(data =X_all_2, columns = X_tr_cal)\n",
    "\n",
    "X_train_2 = X_all_2[:X_tr_len]\n",
    "X_test_2 = X_all_2[X_tr_len:]\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# y_train.loc[y_train['income']==\"<=50K\",'income'] = int(0)\n",
    "# y_train.loc[y_train['income']==\">50K\",'income'] = int(1)\n",
    "\n",
    "# y_test.loc[y_test['income']==\"<=50K\",'income'] = int(0)\n",
    "# y_test.loc[y_test['income']==\">50K\",'income'] = int(1)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "# model = XGBClassifier(learning_rate=0.01,objective= 'binary:logistic',eval_metric = \"merror\", booster='gbtree',use_label_encoder= True,)\n",
    "model.fit(X_train_2,y_train['income'])\n",
    "print(model.score(X_train_2,y_train['income']))\n",
    "print(model.score(X_test_2,y_test['income']))\n",
    "print(model)\n",
    "pred = model.predict(X_test_2)\n",
    "print(pred)\n",
    "pd.DataFrame({\"pred\":pred}).to_csv(\"수험번호.csv\",index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.loc[y_test['income'] == \"<=50K\",'out_put'] = 0\n",
    "y_test.loc[y_test['income'] != \"<=50K\",'out_put'] = 1\n",
    "\n",
    "pred = pd.read_csv(\"수험번호.csv\")\n",
    "\n",
    "pred.loc[pred['pred'] == \"<=50K\",'out_put'] = 0\n",
    "pred.loc[pred['pred'] != \"<=50K\",'out_put'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.7078965758211041 재현율 0.6539703034215623 r2_score 0.29188439527672205 혼동행렬 [[[1013  536]\n",
      "  [ 300 4664]]\n",
      "\n",
      " [[4664  300]\n",
      "  [ 536 1013]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7967675852321349"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"모델점수\",model.score(X_test_2,y_test['income']))\n",
    "aa , bb = y_test['out_put'],pred['out_put']\n",
    "import sklearn.metrics\n",
    "\n",
    "print(\"f1_score\",sklearn.metrics.f1_score(aa , bb), \\\n",
    "\"재현율\",sklearn.metrics.recall_score(aa , bb), \\\n",
    "\"r2_score\",sklearn.metrics.r2_score(aa , bb), \\\n",
    "\"혼동행렬\",sklearn.metrics.multilabel_confusion_matrix(aa , bb))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "sklearn.metrics.roc_auc_score(aa , bb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-4. House Prices (Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[id_name, target])\n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[id_name, target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\house-prices-advanced-regression-techniques\\\\train.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='SalePrice', id_name='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = len(X_train)\n",
    "X_all = pd.concat( [X_train,X_test])\n",
    "\n",
    "X_all_1 = pd.get_dummies(X_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "X_all_col = X_all_1.columns\n",
    "X_all_2 = Scaler.fit_transform(X_all_1)\n",
    "X_all_2 = pd.DataFrame(X_all_2, columns = X_all_col)\n",
    "\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "\n",
    "y_train_1 = y_train.reset_index(drop= True)\n",
    "y_test_1 = y_test.reset_index(drop= True)\n",
    "\n",
    "Train_all = pd.concat([X_train_2,y_train_1],axis = 1)\n",
    "aa = pd.DataFrame(abs(Train_all.corr()['SalePrice'])>0.1)\n",
    "use_columns = aa.loc[aa['SalePrice']==True][:-1].index\n",
    "\n",
    "X_train_3 = X_train_2[use_columns]\n",
    "X_test_3 = X_test_2[use_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-36c7d7f9e1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SalePrice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1065\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mallow_nd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m             \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_regressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         )\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m     )\n\u001b[0;32m    970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    114\u001b[0m             raise ValueError(\n\u001b[0;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m--> 116\u001b[1;33m                     \u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                 )\n\u001b[0;32m    118\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor()\n",
    "model.fit(X_train_3,y_train_1['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8491270042093133\n",
      "0.8502719975650403\n",
      "31124.538902331296\n",
      "29559.060750897082\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(learning_rate = 0.3,booster = \"gblinear\", reg_lambda = 1.2, reg_alpha = 0.2) # @@ gbliner를 이용하자\n",
    "# model = XGBRegressor()\n",
    "model.fit(X_train_3,y_train_1['SalePrice'])\n",
    "\n",
    "print(model.score(X_train_3,y_train_1['SalePrice']))\n",
    "\n",
    "pred = model.predict(X_test_3)\n",
    "\n",
    "pd.DataFrame({\"predict\":pred}).to_csv(\"수험번호.csv\", index = False)\n",
    "# pred\n",
    "\n",
    "print(model.score(X_test_3,y_test_1['SalePrice']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train_1['SalePrice'], model.predict(X_train_3)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test_1['SalePrice'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997592786239734\n",
      "0.9074264332872578\n",
      "1243.2386352266383\n",
      "23242.48788008825\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor(learning_rate = 0.1,booster = \"gblinear\") # @@ gbliner를 이용하자\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train_2,y_train_1['SalePrice'])\n",
    "\n",
    "print(model.score(X_train_2,y_train_1['SalePrice']))\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "# pred\n",
    "\n",
    "print(model.score(X_test_2,y_test_1['SalePrice']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train_1['SalePrice'], model.predict(X_train_2)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test_1['SalePrice'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-5. Insurance_Starter (Tutorial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1070, 7), (268, 7), (1070, 2), (268, 2))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시험환경 세팅 (코드 변경 X)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "    \n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[target])\n",
    "\n",
    "    \n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\archive\\\\insurance.csv\")\n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='charges')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964181737350261\n",
      "0.8120888367117656\n"
     ]
    }
   ],
   "source": [
    "X_train_len = len(X_train)\n",
    "\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "x_train = X_all_1[:X_train_len]\n",
    "x_test = X_all_1[X_train_len:]\n",
    "\n",
    "train_all = pd.concat([x_train,y_train],axis = 1)\n",
    "df1 = pd.DataFrame(abs(train_all.corr()['charges'])>0.007)\n",
    "train_col = df1.loc[df1['charges']==True].index[:-1]\n",
    "X_all_2 = X_all_1[train_col]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Scaler = MinMaxScaler()\n",
    "X_all_3 = Scaler.fit_transform(X_all_2)\n",
    "X_all_3 = pd.DataFrame(X_all_3,columns = train_col)\n",
    "\n",
    "x_train = X_all_3[:X_train_len]\n",
    "x_test = X_all_3[X_train_len:]\n",
    "# y_train.shape, y_test.shape\n",
    "\n",
    "# y_train = y_train.reset_index(drop=True)\n",
    "# y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(x_train,y_train['charges'])\n",
    "print(model.score(x_train,y_train['charges']))\n",
    "pred = model.predict(x_test)\n",
    "pd.DataFrame({\"predict\":pred}).to_csv(\"수험번호.csv\",index= False)\n",
    "\n",
    "print(model.score(x_test,y_test['charges']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9964181737350261\n",
      "0.8120559186498852\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# model = XGBRegressor(learning_rate = 0.1,booster = \"gblinear\") # @@ gbliner를 이용하자??\n",
    "model = XGBRegressor()\n",
    "# model = XGBRegressor(booster = \"gblinear\")\n",
    "model.fit(x_train,y_train['charges'])\n",
    "\n",
    "print(model.score(x_train,y_train['charges']))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "# pred\n",
    "\n",
    "print(model.score(x_test,y_test['charges']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train['charges'], model.predict(x_train)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test['charges'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T2-6. Bike-sharing-demand (Regression)_2[joge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n",
    "    if id_name == \"\":\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "        id_name = 'id'\n",
    "    else:\n",
    "        id_name = id_name\n",
    "    \n",
    "    if null_name != \"\":\n",
    "        df[df == null_name] = np.nan\n",
    "    \n",
    "    X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n",
    "    y_train = X_train[[id_name, target]]\n",
    "    X_train = X_train.drop(columns=[id_name, target])\n",
    "    y_test = X_test[[id_name, target]]\n",
    "    X_test = X_test.drop(columns=[id_name, target])\n",
    "    return X_train, X_test, y_train, y_test \n",
    "    \n",
    "df = pd.read_csv(path+ \"\\\\bike\\\\train.csv\")\n",
    "    \n",
    "X_train, X_test, y_train, y_test = exam_data_load(df, target='count')#, id_name='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = len(X_train)\n",
    "x_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "x_all['datetime'] = pd.to_datetime(x_all['datetime'])\n",
    "\n",
    "\n",
    "x_all_1 = pd.get_dummies(x_all)\n",
    "del x_all_1['datetime']\n",
    "X_columns = x_all_1.columns\n",
    "\n",
    "x_all_1 = pd.DataFrame(x_all_1 , columns = X_columns)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Scaler = StandardScaler()\n",
    "x_all_2 = Scaler.fit_transform(x_all_1)\n",
    "# x_all_2 = pd.DataFrame(x_all_2 , columns = X_columns)\n",
    "\n",
    "x_train =  x_all_2[:X_train_len]\n",
    "x_test = x_all_2[X_train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999352385395511\n",
      "0.9996688076004838\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "model.fit(x_train,y_train['count'])\n",
    "pred = model.predict(x_test)\n",
    "print(model.score(x_train,y_train['count']))\n",
    "\n",
    "print(model.score(x_test,y_test['count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"index\":y_test.id,\"pred\":pred})\n",
    "df = df.set_index(df['index'],drop = True,inplace=False)\n",
    "del df[\"index\"]\n",
    "df.to_csv(\"수험번호.csv\",index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999352385395511\n",
      "0.9996688076004838\n",
      "1.4631759227449768\n",
      "3.246032707799647\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# model = XGBRegressor(learning_rate = 0.1,booster = \"gblinear\") # @@ gbliner를 이용하자??\n",
    "model = XGBRegressor()\n",
    "# model = XGBRegressor(booster = \"gblinear\")\n",
    "model.fit(x_train,y_train['count'])\n",
    "\n",
    "print(model.score(x_train,y_train['count']))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "# pred\n",
    "\n",
    "print(model.score(x_test,y_test['count']))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(y_train['count'], model.predict(x_train)))) # 정답\n",
    "print(np.sqrt(mean_squared_error(y_test['count'], pred))) # 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐글 풀이\n",
    "# # print(X_train.head())\n",
    "# # print(y_train.head())\n",
    "\n",
    "# X_train_len = len(X_train)\n",
    "# X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "\n",
    "# # print(X_all.info())\n",
    "# # print(X_all[\"datetime\"])\n",
    "# # print(X_all.head())\n",
    "# # print(X_all.tail())\n",
    "# # print(X_all.columns)\n",
    "\n",
    "# # print(pd.to_datetime(X_all[\"datetime\"]))\n",
    "# Day_time = X_all[[\"datetime\"]]\n",
    "# X_all = X_all.drop([\"datetime\"],axis = 1)\n",
    "# X_all_1 = pd.get_dummies(X_all)\n",
    "\n",
    "# # X_all_1 = pd.concat([X_all_1,Day_time],axis = 1)\n",
    "# print(X_all_1.head())\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# SS = StandardScaler()\n",
    "# X_all_2 = SS.fit_transform(X_all_1)\n",
    "\n",
    "\n",
    "\n",
    "# X_train_2 = X_all_2[:X_train_len]\n",
    "# X_test_2 = X_all_2[X_train_len:]\n",
    "\n",
    "# from xgboost import XGBRegressor\n",
    "# model = XGBRegressor()\n",
    "\n",
    "# model.fit(X_train_2,y_train[\"count\"])\n",
    "\n",
    "# print(model.score(X_train_2,y_train[\"count\"]))\n",
    "\n",
    "# pred = model.predict(X_test_2)\n",
    "\n",
    "# print(model.score(X_test_2,y_test[\"count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MOCK EXAM1] TYPE2. HR-DATA / 작업형2 모의고사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+ \"\\\\hr_data\\\\X_test.csv\")\n",
    "X_train = pd.read_csv(path+ \"\\\\hr_data\\\\X_train.csv\")\n",
    "y_train = pd.read_csv(path+ \"\\\\hr_data\\\\y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:34:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8521244388767095\n"
     ]
    }
   ],
   "source": [
    "tr_len = len(X_train)\n",
    "x_all = pd.concat([X_train , X_test],axis = 0)\n",
    "\n",
    "del x_all['enrollee_id']\n",
    "\n",
    "x_all_1 = pd.get_dummies(x_all)\n",
    "X_columns = x_all_1.columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # @ \n",
    "Scaler = StandardScaler()\n",
    "Scaler.fit(x_all_1)\n",
    "x_all_2 = Scaler.transform(x_all_1)\n",
    "\n",
    "# x_all_2 = pd.DataFrame(x_all_2 ,columns = X_columns)\n",
    "x_train = x_all_2[:tr_len]\n",
    "x_test = x_all_2[tr_len:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:35:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.8521244388767095\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train , y_train[\"target\"])\n",
    "print(model.score(x_train , y_train[\"target\"]))\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "df = pd.DataFrame({\"predict\":pred})\n",
    "\n",
    "df['index'] = y_train['enrollee_id']\n",
    "df = df.set_index(df[\"index\"],drop = True,inplace = False)\n",
    "df.to_csv(\"수험번호.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9985906670842468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(x_train , y_train[\"target\"])\n",
    "print(model.score(x_train , y_train[\"target\"]))\n",
    "\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AdaBoostClassifier',\n",
       " 'AdaBoostRegressor',\n",
       " 'BaggingClassifier',\n",
       " 'BaggingRegressor',\n",
       " 'BaseEnsemble',\n",
       " 'ExtraTreesClassifier',\n",
       " 'ExtraTreesRegressor',\n",
       " 'GradientBoostingClassifier',\n",
       " 'GradientBoostingRegressor',\n",
       " 'HistGradientBoostingClassifier',\n",
       " 'HistGradientBoostingRegressor',\n",
       " 'IsolationForest',\n",
       " 'RandomForestClassifier',\n",
       " 'RandomForestRegressor',\n",
       " 'RandomTreesEmbedding',\n",
       " 'StackingClassifier',\n",
       " 'StackingRegressor',\n",
       " 'VotingClassifier',\n",
       " 'VotingRegressor',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_bagging',\n",
       " '_base',\n",
       " '_forest',\n",
       " '_gb',\n",
       " '_gb_losses',\n",
       " '_gradient_boosting',\n",
       " '_hist_gradient_boosting',\n",
       " '_iforest',\n",
       " '_stacking',\n",
       " '_voting',\n",
       " '_weight_boosting']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sklearn.ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path+\"\\\\mtcars.csv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시험환경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['총구매액', '최대구매액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기', '주구매상품_가공식품',\n",
      "       '주구매상품_가구', '주구매상품_건강식품', '주구매상품_골프', '주구매상품_구두', '주구매상품_기타',\n",
      "       '주구매상품_남성 캐주얼', '주구매상품_남성 트랜디', '주구매상품_남성정장', '주구매상품_농산물', '주구매상품_대형가전',\n",
      "       '주구매상품_디자이너', '주구매상품_란제리/내의', '주구매상품_명품', '주구매상품_모피/피혁', '주구매상품_보석',\n",
      "       '주구매상품_생활잡화', '주구매상품_섬유잡화', '주구매상품_셔츠', '주구매상품_소형가전', '주구매상품_수산품',\n",
      "       '주구매상품_스포츠', '주구매상품_시티웨어', '주구매상품_식기', '주구매상품_아동', '주구매상품_악기',\n",
      "       '주구매상품_액세서리', '주구매상품_육류', '주구매상품_일용잡화', '주구매상품_젓갈/반찬', '주구매상품_주류',\n",
      "       '주구매상품_주방가전', '주구매상품_주방용품', '주구매상품_차/커피', '주구매상품_축산가공', '주구매상품_침구/수예',\n",
      "       '주구매상품_캐주얼', '주구매상품_커리어', '주구매상품_통신/컴퓨터', '주구매상품_트래디셔널', '주구매상품_피혁잡화',\n",
      "       '주구매상품_화장품', '주구매지점_강남점', '주구매지점_관악점', '주구매지점_광주점', '주구매지점_노원점',\n",
      "       '주구매지점_대구점', '주구매지점_대전점', '주구매지점_동래점', '주구매지점_미아점', '주구매지점_본  점',\n",
      "       '주구매지점_부산본점', '주구매지점_부평점', '주구매지점_분당점', '주구매지점_상인점', '주구매지점_센텀시티점',\n",
      "       '주구매지점_안양점', '주구매지점_영등포점', '주구매지점_울산점', '주구매지점_인천점', '주구매지점_일산점',\n",
      "       '주구매지점_잠실점', '주구매지점_전주점', '주구매지점_창원점', '주구매지점_청량리점', '주구매지점_포항점'],\n",
      "      dtype='object')\n",
      "[0]\tvalidation_0-logloss:0.68450\n",
      "[1]\tvalidation_0-logloss:0.68299\n",
      "[2]\tvalidation_0-logloss:0.68439\n",
      "[3]\tvalidation_0-logloss:0.68495\n",
      "[4]\tvalidation_0-logloss:0.68755\n",
      "[5]\tvalidation_0-logloss:0.69347\n",
      "[6]\tvalidation_0-logloss:0.69704\n",
      "[7]\tvalidation_0-logloss:0.70378\n",
      "[8]\tvalidation_0-logloss:0.70592\n",
      "[9]\tvalidation_0-logloss:0.70792\n",
      "[10]\tvalidation_0-logloss:0.71025\n",
      "[11]\tvalidation_0-logloss:0.71228\n",
      "[12]\tvalidation_0-logloss:0.71192\n",
      "[13]\tvalidation_0-logloss:0.71435\n",
      "[14]\tvalidation_0-logloss:0.71735\n",
      "[15]\tvalidation_0-logloss:0.71939\n",
      "[16]\tvalidation_0-logloss:0.72010\n",
      "[17]\tvalidation_0-logloss:0.72037\n",
      "[18]\tvalidation_0-logloss:0.72317\n",
      "[19]\tvalidation_0-logloss:0.72122\n",
      "[20]\tvalidation_0-logloss:0.72316\n",
      "[21]\tvalidation_0-logloss:0.73039\n",
      "[22]\tvalidation_0-logloss:0.73265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23]\tvalidation_0-logloss:0.73430\n",
      "[24]\tvalidation_0-logloss:0.73754\n",
      "[25]\tvalidation_0-logloss:0.73622\n",
      "[26]\tvalidation_0-logloss:0.73604\n",
      "[27]\tvalidation_0-logloss:0.73687\n",
      "[28]\tvalidation_0-logloss:0.74088\n",
      "[29]\tvalidation_0-logloss:0.74509\n",
      "[30]\tvalidation_0-logloss:0.74589\n",
      "[31]\tvalidation_0-logloss:0.75012\n",
      "[32]\tvalidation_0-logloss:0.75120\n",
      "[33]\tvalidation_0-logloss:0.75682\n",
      "[34]\tvalidation_0-logloss:0.75956\n",
      "[35]\tvalidation_0-logloss:0.75997\n",
      "[36]\tvalidation_0-logloss:0.76197\n",
      "[37]\tvalidation_0-logloss:0.76410\n",
      "[38]\tvalidation_0-logloss:0.76550\n",
      "[39]\tvalidation_0-logloss:0.76965\n",
      "[40]\tvalidation_0-logloss:0.76990\n",
      "[41]\tvalidation_0-logloss:0.77031\n",
      "[42]\tvalidation_0-logloss:0.77140\n",
      "[43]\tvalidation_0-logloss:0.77282\n",
      "[44]\tvalidation_0-logloss:0.77345\n",
      "[45]\tvalidation_0-logloss:0.77511\n",
      "[46]\tvalidation_0-logloss:0.77609\n",
      "[47]\tvalidation_0-logloss:0.77601\n",
      "[48]\tvalidation_0-logloss:0.77851\n",
      "[49]\tvalidation_0-logloss:0.78281\n",
      "[50]\tvalidation_0-logloss:0.78302\n",
      "[51]\tvalidation_0-logloss:0.78227\n",
      "[52]\tvalidation_0-logloss:0.78602\n",
      "[53]\tvalidation_0-logloss:0.78897\n",
      "[54]\tvalidation_0-logloss:0.78942\n",
      "[55]\tvalidation_0-logloss:0.78923\n",
      "[56]\tvalidation_0-logloss:0.79159\n",
      "[57]\tvalidation_0-logloss:0.79359\n",
      "[58]\tvalidation_0-logloss:0.79606\n",
      "[59]\tvalidation_0-logloss:0.79936\n",
      "[60]\tvalidation_0-logloss:0.79899\n",
      "[61]\tvalidation_0-logloss:0.80017\n",
      "[62]\tvalidation_0-logloss:0.80401\n",
      "[63]\tvalidation_0-logloss:0.80420\n",
      "[64]\tvalidation_0-logloss:0.80115\n",
      "[65]\tvalidation_0-logloss:0.80326\n",
      "[66]\tvalidation_0-logloss:0.80379\n",
      "[67]\tvalidation_0-logloss:0.80274\n",
      "[68]\tvalidation_0-logloss:0.80490\n",
      "[69]\tvalidation_0-logloss:0.80713\n",
      "[70]\tvalidation_0-logloss:0.80959\n",
      "[71]\tvalidation_0-logloss:0.81198\n",
      "[72]\tvalidation_0-logloss:0.81726\n",
      "[73]\tvalidation_0-logloss:0.81817\n",
      "[74]\tvalidation_0-logloss:0.81838\n",
      "[75]\tvalidation_0-logloss:0.81848\n",
      "[76]\tvalidation_0-logloss:0.81845\n",
      "[77]\tvalidation_0-logloss:0.82306\n",
      "[78]\tvalidation_0-logloss:0.82501\n",
      "[79]\tvalidation_0-logloss:0.82747\n",
      "[80]\tvalidation_0-logloss:0.82832\n",
      "[81]\tvalidation_0-logloss:0.83084\n",
      "[82]\tvalidation_0-logloss:0.83277\n",
      "[83]\tvalidation_0-logloss:0.83347\n",
      "[84]\tvalidation_0-logloss:0.83674\n",
      "[85]\tvalidation_0-logloss:0.83952\n",
      "[86]\tvalidation_0-logloss:0.84031\n",
      "[87]\tvalidation_0-logloss:0.84092\n",
      "[88]\tvalidation_0-logloss:0.84379\n",
      "[89]\tvalidation_0-logloss:0.84399\n",
      "[90]\tvalidation_0-logloss:0.84632\n",
      "[91]\tvalidation_0-logloss:0.84951\n",
      "[92]\tvalidation_0-logloss:0.84930\n",
      "[93]\tvalidation_0-logloss:0.84932\n",
      "[94]\tvalidation_0-logloss:0.85020\n",
      "[95]\tvalidation_0-logloss:0.85049\n",
      "[96]\tvalidation_0-logloss:0.85775\n",
      "[97]\tvalidation_0-logloss:0.85957\n",
      "[98]\tvalidation_0-logloss:0.86148\n",
      "[99]\tvalidation_0-logloss:0.86094\n",
      "0.4877498388136686 0.30042918454935624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\happy\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9428571428571428\n",
      "0.9242933251891818 0.9166666666666667\n"
     ]
    }
   ],
   "source": [
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "\n",
    "tr_len = len(X_train)\n",
    "X_all = pd.concat([X_test,X_train] , axis= 0)\n",
    "\n",
    "\n",
    "# 환불 여부만 파악\n",
    "# X_all[\"환불여부\"] = None # @ 비교해보기\n",
    "# X_all.loc[X_all[\"환불금액\"] != None,\"환불여부\"] = True\n",
    "\n",
    "X_all = X_all.drop([\"환불금액\"],axis= 1) \n",
    "# print(X_all.head(5))\n",
    "# print(X_all.info())\n",
    "\n",
    "# print(X_all[[\"주구매상품\",\"주구매지점\"]].value_counts())\n",
    "X_all = X_all.drop([\"cust_id\"],axis= 1) \n",
    "# X_all = X_all.drop([\"주구매지점\"],axis= 1) # @ 비교해보기\n",
    "\n",
    "\n",
    "X_all_1 =  pd.get_dummies(X_all)\n",
    "\n",
    "col = X_all_1.columns\n",
    "print(col)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# print(dir(sklearn.preprocessing))\n",
    "import sklearn.preprocessing\n",
    "# print(dir(sklearn.preprocessing))\n",
    "Scaler = StandardScaler()\n",
    "X_all_1 = Scaler.fit_transform(X_all_1)\n",
    "# print(X_all_1.shape)\n",
    "# print(col)\n",
    "X_all_2 = pd.DataFrame(X_all_1,columns = col)\n",
    "\n",
    "x_train = X_all_2[:tr_len]\n",
    "x_test = X_all_2[tr_len:]\n",
    "\n",
    "# @ 검증셋 만들기\n",
    "a = np.floor(len(x_train)/10)\n",
    "b= int(len(x_train)-a)\n",
    "x_train_1 = x_train[:b]\n",
    "x_val_1 = x_train[b:]\n",
    "y_train_1 = y_train[:b]\n",
    "y_val_1 = y_train[b:]\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier # GradientBoostingClassifier # RandomForestClassifier\n",
    "from sklearn.svm import SVC # GradientBoostingClassifier # RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(objective='binary:logistic' ,eval_metric='logloss')\n",
    "# print(help(XGBClassifier))\n",
    "model.fit(x_train_1,y_train_1[\"gender\"],eval_set = [(x_val_1,y_val_1[\"gender\"])])\n",
    "\n",
    "\n",
    "pred= model.predict(x_val_1)\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.roc_auc_score(y_val_1[\"gender\"],pred),sklearn.metrics.f1_score(y_val_1[\"gender\"],pred))\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(objective='binary:logistic' ,eval_metric='logloss')\n",
    "# print(help(XGBClassifier))\n",
    "model.fit(x_train,y_train[\"gender\"])\n",
    "print(model.score(x_train,y_train[\"gender\"]))\n",
    "prob= model.predict_proba(x_test)\n",
    "\n",
    "df = pd.DataFrame({\"custid\":X_test.cust_id,\"gender\":prob[:,1]}) # @@ \n",
    "df.to_csv(\"003000000.csv\",index = False)\n",
    "# df = pd.read_csv(\"003000000.csv\",index = False)\n",
    "# print(df)\n",
    "# # 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "pred= model.predict(x_val_1)\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.roc_auc_score(y_val_1[\"gender\"],pred),sklearn.metrics.f1_score(y_val_1[\"gender\"],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기']\n",
      "0.932\n"
     ]
    }
   ],
   "source": [
    "## xgboost 검증셋 없이 학습\n",
    "\n",
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "# 사용자 코딩\n",
    "\n",
    "# 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "import numpy as np\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_train_len = len(X_train)\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "# print(len(X_all))\n",
    "\n",
    "train_all = pd.concat([X_train,y_train],axis = 1)\n",
    "# print(len(train_all))\n",
    "# print(X_all)\n",
    "\n",
    "\n",
    "#상관분석 후 사용할 컬럼 지정\n",
    "aa = pd.DataFrame(abs(train_all.corr()['gender'])>0.02)\n",
    "use_col = list(aa.loc[aa['gender']==True][:-1].index)\n",
    "print(use_col)\n",
    "######\n",
    "\n",
    "#더미화 및 데이터 확인\n",
    "X_all_1 = X_all[use_col]\n",
    "# print(X_all_1.info())\n",
    "X_all_2 = pd.get_dummies(X_all_1)\n",
    "# print(X_all_2.info())\n",
    "\n",
    "#정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_2)\n",
    "# print(X_all_2)\n",
    "\n",
    "\n",
    "#데이터 프레임 다시 나누기\n",
    "# print(X_train_len)\n",
    "\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "y_train_2 = y_train['gender']\n",
    "# print(y_train_2)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# print(dir(XGBClassifier))\n",
    "# model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "# print(help(model))\n",
    "model.fit(X_train_2,y_train_2)\n",
    "\n",
    "print(model.score(X_train_2,y_train['gender'])) # 모델 점수\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "# prob = model.predict_proba(X_test_2)[:,:1]\n",
    "pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['총구매액', '최대구매액', '환불금액', '내점일수', '내점당구매건수', '주말방문비율', '구매주기']\n",
      "500\n",
      "[0]\tvalidation_0-logloss:0.67263\n",
      "[1]\tvalidation_0-logloss:0.66471\n",
      "[2]\tvalidation_0-logloss:0.66166\n",
      "[3]\tvalidation_0-logloss:0.65971\n",
      "[4]\tvalidation_0-logloss:0.65713\n",
      "[5]\tvalidation_0-logloss:0.65694\n",
      "[6]\tvalidation_0-logloss:0.65899\n",
      "[7]\tvalidation_0-logloss:0.65547\n",
      "[8]\tvalidation_0-logloss:0.65613\n",
      "[9]\tvalidation_0-logloss:0.66075\n",
      "[10]\tvalidation_0-logloss:0.66352\n",
      "[11]\tvalidation_0-logloss:0.66451\n",
      "[12]\tvalidation_0-logloss:0.66231\n",
      "[13]\tvalidation_0-logloss:0.66534\n",
      "[14]\tvalidation_0-logloss:0.66675\n",
      "[15]\tvalidation_0-logloss:0.66750\n",
      "[16]\tvalidation_0-logloss:0.66787\n",
      "[17]\tvalidation_0-logloss:0.66857\n",
      "[18]\tvalidation_0-logloss:0.66907\n",
      "[19]\tvalidation_0-logloss:0.67068\n",
      "[20]\tvalidation_0-logloss:0.67277\n",
      "[21]\tvalidation_0-logloss:0.67333\n",
      "[22]\tvalidation_0-logloss:0.67401\n",
      "[23]\tvalidation_0-logloss:0.67356\n",
      "[24]\tvalidation_0-logloss:0.67435\n",
      "[25]\tvalidation_0-logloss:0.67556\n",
      "[26]\tvalidation_0-logloss:0.67727\n",
      "[27]\tvalidation_0-logloss:0.67919\n",
      "[28]\tvalidation_0-logloss:0.67985\n",
      "[29]\tvalidation_0-logloss:0.67821\n",
      "[30]\tvalidation_0-logloss:0.67879\n",
      "[31]\tvalidation_0-logloss:0.67929\n",
      "[32]\tvalidation_0-logloss:0.68154\n",
      "[33]\tvalidation_0-logloss:0.68416\n",
      "[34]\tvalidation_0-logloss:0.68510\n",
      "[35]\tvalidation_0-logloss:0.68435\n",
      "[36]\tvalidation_0-logloss:0.68383\n",
      "[37]\tvalidation_0-logloss:0.68472\n",
      "[38]\tvalidation_0-logloss:0.68548\n",
      "[39]\tvalidation_0-logloss:0.68400\n",
      "[40]\tvalidation_0-logloss:0.68376\n",
      "[41]\tvalidation_0-logloss:0.68501\n",
      "[42]\tvalidation_0-logloss:0.68513\n",
      "[43]\tvalidation_0-logloss:0.68587\n",
      "[44]\tvalidation_0-logloss:0.68688\n",
      "[45]\tvalidation_0-logloss:0.68703\n",
      "[46]\tvalidation_0-logloss:0.68733\n",
      "[47]\tvalidation_0-logloss:0.68759\n",
      "[48]\tvalidation_0-logloss:0.68833\n",
      "[49]\tvalidation_0-logloss:0.68804\n",
      "[50]\tvalidation_0-logloss:0.69443\n",
      "[51]\tvalidation_0-logloss:0.69540\n",
      "[52]\tvalidation_0-logloss:0.69973\n",
      "[53]\tvalidation_0-logloss:0.70026\n",
      "[54]\tvalidation_0-logloss:0.70025\n",
      "[55]\tvalidation_0-logloss:0.70445\n",
      "[56]\tvalidation_0-logloss:0.70515\n",
      "[57]\tvalidation_0-logloss:0.70639\n",
      "[58]\tvalidation_0-logloss:0.70671\n",
      "[59]\tvalidation_0-logloss:0.70804\n",
      "[60]\tvalidation_0-logloss:0.70905\n",
      "[61]\tvalidation_0-logloss:0.71108\n",
      "[62]\tvalidation_0-logloss:0.71275\n",
      "[63]\tvalidation_0-logloss:0.71339\n",
      "[64]\tvalidation_0-logloss:0.71389\n",
      "[65]\tvalidation_0-logloss:0.71556\n",
      "[66]\tvalidation_0-logloss:0.71705\n",
      "[67]\tvalidation_0-logloss:0.71690\n",
      "[68]\tvalidation_0-logloss:0.71889\n",
      "[69]\tvalidation_0-logloss:0.72016\n",
      "[70]\tvalidation_0-logloss:0.72524\n",
      "[71]\tvalidation_0-logloss:0.72626\n",
      "[72]\tvalidation_0-logloss:0.72749\n",
      "[73]\tvalidation_0-logloss:0.72738\n",
      "[74]\tvalidation_0-logloss:0.72907\n",
      "[75]\tvalidation_0-logloss:0.72815\n",
      "[76]\tvalidation_0-logloss:0.72927\n",
      "[77]\tvalidation_0-logloss:0.72959\n",
      "[78]\tvalidation_0-logloss:0.73216\n",
      "[79]\tvalidation_0-logloss:0.73368\n",
      "[80]\tvalidation_0-logloss:0.73567\n",
      "[81]\tvalidation_0-logloss:0.73607\n",
      "[82]\tvalidation_0-logloss:0.73542\n",
      "[83]\tvalidation_0-logloss:0.73562\n",
      "[84]\tvalidation_0-logloss:0.73722\n",
      "[85]\tvalidation_0-logloss:0.73665\n",
      "[86]\tvalidation_0-logloss:0.73886\n",
      "[87]\tvalidation_0-logloss:0.74023\n",
      "[88]\tvalidation_0-logloss:0.73986\n",
      "[89]\tvalidation_0-logloss:0.74101\n",
      "[90]\tvalidation_0-logloss:0.74286\n",
      "[91]\tvalidation_0-logloss:0.74326\n",
      "[92]\tvalidation_0-logloss:0.74367\n",
      "[93]\tvalidation_0-logloss:0.74496\n",
      "[94]\tvalidation_0-logloss:0.74469\n",
      "[95]\tvalidation_0-logloss:0.74796\n",
      "[96]\tvalidation_0-logloss:0.74825\n",
      "[97]\tvalidation_0-logloss:0.74855\n",
      "[98]\tvalidation_0-logloss:0.74899\n",
      "[99]\tvalidation_0-logloss:0.75178\n",
      "0.943\n",
      "0.5644519875538694 0.4350282485875706\n"
     ]
    }
   ],
   "source": [
    "######################################################################### \n",
    "## xgboost 검증셋 추가\n",
    "\n",
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "# 사용자 코딩\n",
    "\n",
    "# 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "import numpy as np\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_train_len = len(X_train)\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "# print(len(X_all))\n",
    "\n",
    "train_all = pd.concat([X_train,y_train],axis = 1)\n",
    "# print(len(train_all))\n",
    "# print(X_all)\n",
    "\n",
    "\n",
    "#상관분석 후 사용할 컬럼 지정\n",
    "aa = pd.DataFrame(abs(train_all.corr()['gender'])>0.02)\n",
    "use_col = list(aa.loc[aa['gender']==True][:-1].index)\n",
    "print(use_col)\n",
    "######\n",
    "\n",
    "#더미화 및 데이터 확인\n",
    "X_all_1 = X_all[use_col]\n",
    "# print(X_all_1.info())\n",
    "X_all_2 = pd.get_dummies(X_all_1)\n",
    "# print(X_all_2.info())\n",
    "\n",
    "#정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_2)\n",
    "# print(X_all_2)\n",
    "\n",
    "\n",
    "#데이터 프레임 다시 나누기\n",
    "# print(X_train_len)\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "y_train_2 = y_train['gender']\n",
    "\n",
    "x_validation = X_train_2[-500:]\n",
    "y_validation = y_train_2[-500:]\n",
    "print(len(x_validation))\n",
    "\n",
    "X_train_2 = X_train_2[:-500]\n",
    "y_train_2 = y_train_2[:-500]\n",
    "\n",
    "# print(y_train_2)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# print(dir(XGBClassifier))\n",
    "# model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "# print(help(model))\n",
    "# model.fit(X_train_2,y_train_2)\n",
    "model.fit(X_train_2,y_train_2,eval_set=[(x_validation, y_validation)])\n",
    "\n",
    "print(model.score(X_train_2,y_train_2)) # 모델 점수\n",
    "\n",
    "pred = model.predict(X_test_2)\n",
    "# prob = model.predict_proba(X_test_2)[:,:1]\n",
    "pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pred= model.predict(x_validation)\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.roc_auc_score(y_validation,pred),sklearn.metrics.f1_score(y_validation,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################### \n",
    "## xgboost 검증셋 추가\n",
    "\n",
    "# 출력을 원하실 경우 print() 함수 활용\n",
    "# 예시) print(df.head())\n",
    "\n",
    "# getcwd(), chdir() 등 작업 폴더 설정 불필요\n",
    "# 파일 경로 상 내부 드라이브 경로(C: 등) 접근 불가\n",
    "\n",
    "# 데이터 파일 읽기 예제\n",
    "import pandas as pd\n",
    "X_test = pd.read_csv(path+\"\\\\X_test.csv\",encoding=\"euc-kr\")\n",
    "\n",
    "X_train = pd.read_csv(path+\"\\\\X_train.csv\",encoding=\"euc-kr\")\n",
    "y_train = pd.read_csv(path+\"\\\\y_train.csv\",encoding=\"euc-kr\")\n",
    "# 사용자 코딩\n",
    "\n",
    "# 답안 제출 참고\n",
    "# 아래 코드 예측변수와 수험번호를 개인별로 변경하여 활용\n",
    "# pd.DataFrame({'cust_id': X_test.cust_id, 'gender': pred}).to_csv('003000000.csv', index=False)\n",
    "\n",
    "import numpy as np\n",
    "X_all = pd.concat([X_train,X_test],axis = 0)\n",
    "X_train_len = len(X_train)\n",
    "\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))\n",
    "# print(len(X_all))\n",
    "\n",
    "train_all = pd.concat([X_train,y_train],axis = 1)\n",
    "# print(len(train_all))\n",
    "# print(X_all)\n",
    "\n",
    "\n",
    "#상관분석 후 사용할 컬럼 지정\n",
    "aa = pd.DataFrame(abs(train_all.corr()['gender'])>0.02)\n",
    "use_col = list(aa.loc[aa['gender']==True][:-1].index)\n",
    "print(use_col)\n",
    "######\n",
    "\n",
    "#더미화 및 데이터 확인\n",
    "X_all_1 = X_all[use_col]\n",
    "X_all_1 = X_all_1.drop([\"환불금액\"], axis = 1)\n",
    "print(X_all_1.info())\n",
    "X_all_2 = pd.get_dummies(X_all_1)\n",
    "# print(X_all_2.info())\n",
    "\n",
    "#정규화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_all_2 = SS.fit_transform(X_all_2)\n",
    "# print(X_all_2)\n",
    "\n",
    "\n",
    "#데이터 프레임 다시 나누기\n",
    "# print(X_train_len)\n",
    "X_test_2 = X_all_2[X_train_len:]\n",
    "X_train_2 = X_all_2[:X_train_len]\n",
    "y_train_2 = y_train\n",
    "\n",
    "x_validation = X_train_2[-500:]\n",
    "y_validation = y_train_2[-500:]\n",
    "print(len(x_validation))\n",
    "\n",
    "X_train_2 = X_train_2[:-500]\n",
    "y_train_2 = y_train_2[:-500]\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier # GradientBoostingClassifier # RandomForestClassifier\n",
    "# print(dir(XGBClassifier))\n",
    "# model = XGBClassifier(eval_metric = \"logloss\",use_label_encoder = False)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_2,y_train_2[\"gender\"])\n",
    "\n",
    "pred= model.predict(x_validation)\n",
    "\n",
    "import sklearn.metrics\n",
    "print(sklearn.metrics.roc_auc_score(y_validation[\"gender\"],pred),sklearn.metrics.f1_score(y_validation[\"gender\"],pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
