{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn\n",
      "\n",
      "DESCRIPTION\n",
      "    Machine learning module for Python\n",
      "    ==================================\n",
      "    \n",
      "    sklearn is a Python module integrating classical machine\n",
      "    learning algorithms in the tightly-knit world of scientific Python\n",
      "    packages (numpy, scipy, matplotlib).\n",
      "    \n",
      "    It aims to provide simple and efficient solutions to learning problems\n",
      "    that are accessible to everybody and reusable in various contexts:\n",
      "    machine-learning as a versatile tool for science and engineering.\n",
      "    \n",
      "    See http://scikit-learn.org for complete documentation.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __check_build (package)\n",
      "    _build_utils (package)\n",
      "    _config\n",
      "    _distributor_init\n",
      "    _isotonic\n",
      "    _loss (package)\n",
      "    _min_dependencies\n",
      "    base\n",
      "    calibration\n",
      "    cluster (package)\n",
      "    compose (package)\n",
      "    conftest\n",
      "    covariance (package)\n",
      "    cross_decomposition (package)\n",
      "    datasets (package)\n",
      "    decomposition (package)\n",
      "    discriminant_analysis\n",
      "    dummy\n",
      "    ensemble (package)\n",
      "    exceptions\n",
      "    experimental (package)\n",
      "    externals (package)\n",
      "    feature_extraction (package)\n",
      "    feature_selection (package)\n",
      "    gaussian_process (package)\n",
      "    impute (package)\n",
      "    inspection (package)\n",
      "    isotonic\n",
      "    kernel_approximation\n",
      "    kernel_ridge\n",
      "    linear_model (package)\n",
      "    manifold (package)\n",
      "    metrics (package)\n",
      "    mixture (package)\n",
      "    model_selection (package)\n",
      "    multiclass\n",
      "    multioutput\n",
      "    naive_bayes\n",
      "    neighbors (package)\n",
      "    neural_network (package)\n",
      "    pipeline\n",
      "    preprocessing (package)\n",
      "    random_projection\n",
      "    semi_supervised (package)\n",
      "    setup\n",
      "    svm (package)\n",
      "    tests (package)\n",
      "    tree (package)\n",
      "    utils (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clone(estimator, *, safe=True)\n",
      "        Construct a new unfitted estimator with the same parameters.\n",
      "        \n",
      "        Clone does a deep copy of the model in an estimator\n",
      "        without actually copying attached data. It returns a new estimator\n",
      "        with the same parameters that has not been fitted on any data.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : {list, tuple, set} of estimator instance or a single             estimator instance\n",
      "            The estimator or group of estimators to be cloned.\n",
      "        safe : bool, default=True\n",
      "            If safe is False, clone will fall back to a deep copy on objects\n",
      "            that are not estimators.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        estimator : object\n",
      "            The deep copy of the input, an estimator if input is an estimator.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the estimator's `random_state` parameter is an integer (or if the\n",
      "        estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
      "        returned: the clone and the original estimator will give the exact same\n",
      "        results. Otherwise, *statistical clone* is returned: the clone might\n",
      "        return different results from the original estimator. More details can be\n",
      "        found in :ref:`randomness`.\n",
      "    \n",
      "    config_context(*, assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Context manager for global scikit-learn configuration.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error. If None, the existing value won't change.\n",
      "            The default value is False.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. If None, the existing value won't change.\n",
      "            The default value is 1024.\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()', but would print\n",
      "            'SVC(C=1.0, cache_size=200, ...)' with all the non-changed parameters\n",
      "            when False. If None, the existing value won't change.\n",
      "            The default value is True.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Default changed from False to True.\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. If None, the existing value won't change.\n",
      "            The default value is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        None.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        All settings, not just those presently modified, will be returned to\n",
      "        their previous values when the context manager is exited.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import sklearn\n",
      "        >>> from sklearn.utils.validation import assert_all_finite\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     assert_all_finite([float('nan')])\n",
      "        >>> with sklearn.config_context(assume_finite=True):\n",
      "        ...     with sklearn.config_context(assume_finite=False):\n",
      "        ...         assert_all_finite([float('nan')])\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueError: Input contains NaN...\n",
      "    \n",
      "    get_config()\n",
      "        Retrieve current values for configuration set by :func:`set_config`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        config : dict\n",
      "            Keys are parameter names that can be passed to :func:`set_config`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        set_config : Set global scikit-learn configuration.\n",
      "    \n",
      "    set_config(assume_finite=None, working_memory=None, print_changed_only=None, display=None)\n",
      "        Set global scikit-learn configuration\n",
      "        \n",
      "        .. versionadded:: 0.19\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        assume_finite : bool, default=None\n",
      "            If True, validation for finiteness will be skipped,\n",
      "            saving time, but leading to potential crashes. If\n",
      "            False, validation for finiteness will be performed,\n",
      "            avoiding error.  Global default: False.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            If set, scikit-learn will attempt to limit the size of temporary arrays\n",
      "            to this number of MiB (per job when parallelised), often saving both\n",
      "            computation time and memory on expensive operations that can be\n",
      "            performed in chunks. Global default: 1024.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        print_changed_only : bool, default=None\n",
      "            If True, only the parameters that were set to non-default\n",
      "            values will be printed when printing an estimator. For example,\n",
      "            ``print(SVC())`` while True will only print 'SVC()' while the default\n",
      "            behaviour would be to print 'SVC(C=1.0, cache_size=200, ...)' with\n",
      "            all the non-changed parameters.\n",
      "        \n",
      "            .. versionadded:: 0.21\n",
      "        \n",
      "        display : {'text', 'diagram'}, default=None\n",
      "            If 'diagram', estimators will be displayed as a diagram in a Jupyter\n",
      "            lab or notebook context. If 'text', estimators will be displayed as\n",
      "            text. Default is 'text'.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        config_context : Context manager for global scikit-learn configuration.\n",
      "        get_config : Retrieve current values of the global configuration.\n",
      "    \n",
      "    show_versions()\n",
      "        Print useful debugging information\"\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "\n",
      "DATA\n",
      "    __SKLEARN_SETUP__ = False\n",
      "    __all__ = ['calibration', 'cluster', 'covariance', 'cross_decompositio...\n",
      "\n",
      "VERSION\n",
      "    1.0.2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\happy\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn)\n",
    "\n",
    "# calibration\n",
    "# cluster (package)\n",
    "# compose (package)\n",
    "# conftest\n",
    "# covariance (package)\n",
    "# cross_decomposition (package)\n",
    "# datasets (package)\n",
    "# decomposition (package)\n",
    "# discriminant_analysis\n",
    "# dummy\n",
    "# ensemble (package)\n",
    "# exceptions\n",
    "# experimental (package)\n",
    "# externals (package)\n",
    "# feature_extraction (package)\n",
    "# feature_selection (package)\n",
    "# gaussian_process (package)\n",
    "# impute (package)\n",
    "# inspection (package)\n",
    "# isotonic\n",
    "# kernel_approximation\n",
    "# kernel_ridge\n",
    "# linear_model (package)\n",
    "# manifold (package)\n",
    "# metrics (package)\n",
    "# mixture (package)\n",
    "# model_selection (package)\n",
    "# multiclass\n",
    "# multioutput\n",
    "# naive_bayes\n",
    "# neighbors (package)\n",
    "# neural_network (package)\n",
    "# pipeline\n",
    "# preprocessing (package)\n",
    "# random_projection\n",
    "# semi_supervised (package)\n",
    "# setup\n",
    "# svm (package)\n",
    "# tests (package)\n",
    "# tree (package)\n",
    "# utils (package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ConfusionMatrixDisplay',\n",
       " 'DetCurveDisplay',\n",
       " 'PrecisionRecallDisplay',\n",
       " 'RocCurveDisplay',\n",
       " 'SCORERS',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_base',\n",
       " '_classification',\n",
       " '_pairwise_fast',\n",
       " '_plot',\n",
       " '_ranking',\n",
       " '_regression',\n",
       " '_scorer',\n",
       " 'accuracy_score',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'auc',\n",
       " 'average_precision_score',\n",
       " 'balanced_accuracy_score',\n",
       " 'brier_score_loss',\n",
       " 'calinski_harabasz_score',\n",
       " 'check_scoring',\n",
       " 'classification_report',\n",
       " 'cluster',\n",
       " 'cohen_kappa_score',\n",
       " 'completeness_score',\n",
       " 'confusion_matrix',\n",
       " 'consensus_score',\n",
       " 'coverage_error',\n",
       " 'd2_tweedie_score',\n",
       " 'davies_bouldin_score',\n",
       " 'dcg_score',\n",
       " 'det_curve',\n",
       " 'euclidean_distances',\n",
       " 'explained_variance_score',\n",
       " 'f1_score',\n",
       " 'fbeta_score',\n",
       " 'fowlkes_mallows_score',\n",
       " 'get_scorer',\n",
       " 'hamming_loss',\n",
       " 'hinge_loss',\n",
       " 'homogeneity_completeness_v_measure',\n",
       " 'homogeneity_score',\n",
       " 'jaccard_score',\n",
       " 'label_ranking_average_precision_score',\n",
       " 'label_ranking_loss',\n",
       " 'log_loss',\n",
       " 'make_scorer',\n",
       " 'matthews_corrcoef',\n",
       " 'max_error',\n",
       " 'mean_absolute_error',\n",
       " 'mean_absolute_percentage_error',\n",
       " 'mean_gamma_deviance',\n",
       " 'mean_pinball_loss',\n",
       " 'mean_poisson_deviance',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_log_error',\n",
       " 'mean_tweedie_deviance',\n",
       " 'median_absolute_error',\n",
       " 'multilabel_confusion_matrix',\n",
       " 'mutual_info_score',\n",
       " 'nan_euclidean_distances',\n",
       " 'ndcg_score',\n",
       " 'normalized_mutual_info_score',\n",
       " 'pair_confusion_matrix',\n",
       " 'pairwise',\n",
       " 'pairwise_distances',\n",
       " 'pairwise_distances_argmin',\n",
       " 'pairwise_distances_argmin_min',\n",
       " 'pairwise_distances_chunked',\n",
       " 'pairwise_kernels',\n",
       " 'plot_confusion_matrix',\n",
       " 'plot_det_curve',\n",
       " 'plot_precision_recall_curve',\n",
       " 'plot_roc_curve',\n",
       " 'precision_recall_curve',\n",
       " 'precision_recall_fscore_support',\n",
       " 'precision_score',\n",
       " 'r2_score',\n",
       " 'rand_score',\n",
       " 'recall_score',\n",
       " 'roc_auc_score',\n",
       " 'roc_curve',\n",
       " 'silhouette_samples',\n",
       " 'silhouette_score',\n",
       " 'top_k_accuracy_score',\n",
       " 'v_measure_score',\n",
       " 'zero_one_loss']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cluster,dummy,ensemble ,linear_model ,metrics ,model_selection ,preprocessing ,svm \n",
    "dir(sklearn.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Booster',\n",
       " 'CVBooster',\n",
       " 'DaskLGBMClassifier',\n",
       " 'DaskLGBMRanker',\n",
       " 'DaskLGBMRegressor',\n",
       " 'Dataset',\n",
       " 'LGBMClassifier',\n",
       " 'LGBMModel',\n",
       " 'LGBMRanker',\n",
       " 'LGBMRegressor',\n",
       " 'Path',\n",
       " 'Sequence',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_version_path',\n",
       " 'basic',\n",
       " 'callback',\n",
       " 'compat',\n",
       " 'create_tree_digraph',\n",
       " 'cv',\n",
       " 'dask',\n",
       " 'early_stopping',\n",
       " 'engine',\n",
       " 'libpath',\n",
       " 'log_evaluation',\n",
       " 'plot_importance',\n",
       " 'plot_metric',\n",
       " 'plot_split_value_histogram',\n",
       " 'plot_tree',\n",
       " 'plotting',\n",
       " 'print_evaluation',\n",
       " 'record_evaluation',\n",
       " 'register_logger',\n",
       " 'reset_parameter',\n",
       " 'sklearn',\n",
       " 'train']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "dir(lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사결정 나무 ## 분류 회귀 둘다 다름 ㅅㅂ\n",
    "from sklearn import BaseDecisionTree,dummy,DecisionTreeClassifier ,DecisionTreeRegressor ,ExtraTreeClassifier ,ExtraTreeClassifier ,ExtraTreeRegressor\n",
    "dir(sklearn.tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류모델\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,HistGradientBoostingClassifier,RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier # light xgboost\n",
    "import lightgbm as lgb # lgb자체로도 학습이 가능 !! # https://for-my-wealthy-life.tistory.com/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀모델\n",
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor,HistGradientBoostingRegressor,RandomForestRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from lightgbm import LGBMRegressor # light xgboost\n",
    "import lightgbm as lgb # lgb자체로도 학습이 가능 !! # https://for-my-wealthy-life.tistory.com/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨인코더(다분류), 스케일링\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function mean_absolute_error in module sklearn.metrics._regression:\n",
      "\n",
      "mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "    Mean absolute error regression loss.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "        Estimated target values.\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        Defines aggregating of multiple output values.\n",
      "        Array-like value defines weights used to average errors.\n",
      "    \n",
      "        'raw_values' :\n",
      "            Returns a full set of errors in case of multioutput input.\n",
      "    \n",
      "        'uniform_average' :\n",
      "            Errors of all outputs are averaged with uniform weight.\n",
      "    \n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    loss : float or ndarray of floats\n",
      "        If multioutput is 'raw_values', then mean absolute error is returned\n",
      "        for each output separately.\n",
      "        If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "        weighted average of all output errors is returned.\n",
      "    \n",
      "        MAE output is non-negative floating point. The best value is 0.0.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import mean_absolute_error\n",
      "    >>> y_true = [3, -0.5, 2, 7]\n",
      "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "    >>> mean_absolute_error(y_true, y_pred)\n",
      "    0.5\n",
      "    >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "    >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "    >>> mean_absolute_error(y_true, y_pred)\n",
      "    0.75\n",
      "    >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "    array([0.5, 1. ])\n",
      "    >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "    0.85...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 정확도 측정지표\n",
    "from sklearn.metrics import accuracy_score, f1_score, r2_score , mean_absolute_error, mean_squared_error\n",
    "help(mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer,LabelEncoder,StandardScaler\n",
    "\n",
    "# 이부분 class로 변환할까? 나중에 정해진 범위까지만 데이터 스케일링 할 수도 있음.\n",
    "# def MinMaxScaler_fun(df): #맞춤 변환 함수와 그에 맞는 데이터를 돌려줌\n",
    "#     col = list(df.columns)\n",
    "#     Scaler = MinMaxScaler()\n",
    "#     Scaler.fit(df)\n",
    "#     df = Scaler.transform(df)\n",
    "# #     df = Scaler_X.fit_transform(df)\n",
    "#     df = pd.DataFrame(df, columns = X_col)\n",
    "#     return df, Scaler\n",
    "# X_data ,Scaler = MinMaxScaler_fun(X_data)\n",
    "# Y_data , Y_data_Scaler = MinMaxScaler_fun(Y_data)\n",
    "\n",
    "X_col = list(X_data.columns)\n",
    "Scaler = MinMaxScaler()\n",
    "Scaler.fit(X_data)\n",
    "X_data = Scaler.transform(X_data)\n",
    "#     df = Scaler_X.fit_transform(df)\n",
    "X_data = pd.DataFrame(X_data, columns = X_col)\n",
    "    \n",
    "# 모데링 잘나오게만 할거면 이렇게 하면 됨\n",
    "\n",
    "\n",
    "\n",
    "# @@ 나중에 여기에 검증셋으로 나누어서 하는 모델도 추가."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대최소 정규화\n",
    "Y_col = list(Y_data.columns)\n",
    "\n",
    "MAX_val = float(max(Y_data.values))\n",
    "MIN_val = float(min(Y_data.values))\n",
    "# 변환(man,max 변환을 또해주면 기존값을 잃어버림.)\n",
    "Y_data[Y_col] = Y_data[Y_col].apply(lambda x : (x - MIN_val)/(MAX_val - MIN_val))\n",
    "# 역변환\n",
    "# Y_data[Y_col] = Y_data[Y_col].apply(lambda x : (x * (MAX_val - MIN_val) + MIN_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test_WINDOW) #예측값\n",
    "actual = np.asarray(y_test_WINDOW)\n",
    "\n",
    "pred = pd.DataFrame(pred,columns = [\"예측값\"]).apply(lambda x : (x * (MAX_val - MIN_val) + MIN_val)) # 원래 값 복원\n",
    "actual = pd.DataFrame(actual,columns = [\"실제값\"]).apply(lambda x : (x * (MAX_val - MIN_val) + MIN_val)) # 예측 값 복원\n",
    "\n",
    "print(pred.shape, actual.shape)\n",
    "\n",
    "i = 0\n",
    "j = len(pred)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(actual[i:j], label='actual')\n",
    "plt.plot(pred[i:j], label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "# model = XGBRegressor(eval_metric = \"rmse\", booster='gblinear',\n",
    "#             base_score=0.5,  max_depth=6, learning_rate=0.001, n_estimators=100 ,reg_alpha=1, reg_lambda=1,\n",
    "#             colsample_bylevel=1,colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
    "#             importance_type='gain', interaction_constraints='', max_delta_step=0, \n",
    "#             min_child_weight=1, missing='nan', monotone_constraints='()',\n",
    "#             n_jobs=0, num_parallel_tree=1, random_state=0,\n",
    "#             scale_pos_weight=1, subsample=1,\n",
    "#             tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "model = XGBRegressor(eval_metric = 'rmse',booster='gblinear',\n",
    "                    use_label_encoder = False )\n",
    "\n",
    "model.fit(x_train,y_train,\n",
    "           verbose=True,\n",
    "           early_stopping_rounds=10,\n",
    "           eval_metric='rmse',\n",
    "#            eval_set=[(x_validation, y_validation)])\n",
    "          eval_set=[(x_test, y_test)])\n",
    "\n",
    "pred = model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-7e627e3e5da9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#검증\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mactual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_train)[:] #검증\n",
    "actual = np.asarray(y_train)[:]\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "i = 0\n",
    "j = len(pred)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.title('train_data')\n",
    "plt.plot(actual[i:j], label='actual')\n",
    "plt.plot(pred[i:j], label='prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"모델 설명력 (학습)\",model.score(x_train,y_train[target]))\n",
    "print(\"모델 설명력 (검증)\",model.score(x_test,y_test[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test) #예측값\n",
    "actual = np.asarray(y_test)\n",
    "\n",
    "pred = pd.DataFrame(pred,columns = [\"예측값\"]).apply(lambda x : (x * (MAX_val - MIN_val) + MIN_val)) # 원래 값 복원\n",
    "actual = pd.DataFrame(actual,columns = [\"실제값\"]).apply(lambda x : (x * (MAX_val - MIN_val) + MIN_val)) # 예측 값 복원\n",
    "\n",
    "print(pred.shape, actual.shape)\n",
    "\n",
    "i = 0\n",
    "j = len(pred)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.title('test_data')\n",
    "plt.plot(actual[i:j], label='actual')\n",
    "plt.plot(pred[i:j], label='prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"모델 설명력 (학습)\",model.score(x_train,y_train[target]))\n",
    "print(\"모델 설명력 (검증)\",model.score(x_test,y_test[target]))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print(\"MSE\",mean_squared_error(actual[\"실제값\"],pred[\"예측값\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'C:\\\\Users\\\\Happy\\\\Desktop\\논문용\\\\krwusdtdata'\n",
    "filename = os.path.join(model_path,'USDT_xgb.model')\n",
    "\n",
    "# 모델 저장\n",
    "model.save_model(filename)\n",
    "\n",
    "# 모델 불러오기\n",
    "new_xgb_model =  XGBRegressor(eval_metric = 'rmse',booster='gblinear',use_label_encoder = False ) # 모델 초기화\n",
    "new_xgb_model.load_model(filename) # 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def leaky_relu(x, leak=0.01, name=\"leaky_relu\"):\n",
    "    with tf.name_scope(name):\n",
    "        # return tf.maximum(x, leak * x)\n",
    "        return tf.maximum(0, x) + leak * tf.minimum(0, x)\n",
    "\n",
    "\n",
    "def lRelu(x, leak=0.01, name=\"leaky_relu\"):\n",
    "    return leaky_relu(x, leak, name)\n",
    "\n",
    "\n",
    "def elu(x, name=\"softplus\"):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.nn.elu(x)\n",
    "\n",
    "\n",
    "def absolute_relu(x, name=\"absolute_relu\"):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.maximum(x, -x)\n",
    "\n",
    "\n",
    "def aRelu(x, name=\"absolute_relu\"):\n",
    "    return absolute_relu(x, name)\n",
    "\n",
    "\n",
    "def square_act(x, name=\"square_act\"):\n",
    "    with tf.name_scope(name):\n",
    "        return x*x\n",
    "\n",
    "\n",
    "def square(x, name=\"square_act\"):\n",
    "    return square_act(x, name)\n",
    "\n",
    "\n",
    "def linear_act(x, name=\"linear_act\"):\n",
    "    with tf.name_scope(name):\n",
    "        return x\n",
    "\n",
    "\n",
    "def linear(x, name=\"linear_act\"):\n",
    "    return linear_act(x, name)\n",
    "\n",
    "\n",
    "def parametric_relu(x, name=\"P_relu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        alpha = tf.get_variable('a', x.get_shape()[-1], initializer=tf.constant_initializer(0.01), dtype=tf.float32)\n",
    "        return tf.maximum(0.0, x) + tf.minimum(0.0, alpha * x)\n",
    "\n",
    "\n",
    "def pRelu(x, name=\"P_relu\"):\n",
    "    return parametric_relu(x, name)\n",
    "\n",
    "\n",
    "def maxout(x, num_param=5, name=\"maxout\"):\n",
    "    with tf.variable_scope(name):\n",
    "        output = []\n",
    "        for i in range(num_param):\n",
    "            name = 'w_%d' % i\n",
    "            w = tf.get_variable(name, x.get_shape()[-1], initializer=tf.constant_initializer(1.0 * (i-num_param/2)))\n",
    "            name = 'b_%d' % i\n",
    "            b = tf.get_variable(name, x.get_shape()[-1], initializer=tf.constant_initializer(i-num_param/2))\n",
    "            out = x*w + b\n",
    "            output.append(out)\n",
    "\n",
    "        ret = tf.reduce_max(output, 0)\n",
    "        return ret\n",
    "\n",
    "\n",
    "def th_relu(x, th=0.01, name=\"th_relu\"):\n",
    "    with tf.name_scope(name):\n",
    "        return tf.select(tf.less(x, th), tf.zeros_like(x), x)\n",
    "\n",
    "\n",
    "def tRelu(x, th=0.01, name=\"the_relu\"):\n",
    "    return th_relu(x, th, name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
